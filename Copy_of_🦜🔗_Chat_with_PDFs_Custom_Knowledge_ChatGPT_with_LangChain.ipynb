{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1GI7Fo8Y7x"
      },
      "source": [
        "# **Custom Knowledge ChatGPT with LangChain - Chat with PDFs**\n",
        "\n",
        "**By Liam Ottley:**  [YouTube](https://youtube.com/@LiamOttley)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "0.   Installs, Imports and API Keys\n",
        "1.   Loading PDFs and chunking with LangChain\n",
        "2.   Embedding text and storing embeddings\n",
        "3.   Creating retrieval function\n",
        "4.   Creating chatbot with chat memory (OPTIONAL)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q24Y-g6h-Bg0"
      },
      "source": [
        "# 0. Installs, Imports and API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk2J2sYYjTkM",
        "outputId": "89cf9a50-c1be-443d-c00a-d982d1f47b3f"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL FIRST!\n",
        "!pip install -q langchain==0.0.150 pypdf pandas matplotlib tiktoken textract transformers openai faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "l-uszlwN641q"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3d2f3a646554fa3830d816b02135356",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> ')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import langchain\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "E2Buv5Y0uFr8"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLULMPXa-Hu8"
      },
      "source": [
        "# 1. Loading PDFs and chunking with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH546j3nkFwX",
        "outputId": "ae2e1c82-e5f3-4dd0-df85-b58288e9e47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content=\"logo \\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat's new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started  Tutorial  What's new  Glossary  Development  FAQ Support  Related packages  Roadmap  Governance  About us  GitHub  Other Versions and Download\\n G o\\n Toggle Menu\\nPrevUp Next\\nscikit-learn 1.3.2\\nOther versions\\nPlease cite us  if you use the software.\\nsklearn.tree .DecisionT reeClassifier\\nDecisionTreeClassifier\\nDecisionTreeClassifier.apply\\nDecisionTreeClassifier.cost_complexity_pruning_path\\nDecisionTreeClassifier.decision_path\\nDecisionTreeClassifier.feature_importances_\\nDecisionTreeClassifier.fit\\nDecisionTreeClassifier.get_depth\\nDecisionTreeClassifier.get_metadata_routing\\nDecisionTreeClassifier.get_n_leaves\\nDecisionTreeClassifier.get_params\\nDecisionTreeClassifier.predict\\nDecisionTreeClassifier.predict_log_proba\\nDecisionTreeClassifier.predict_proba\\nDecisionTreeClassifier.score\\nDecisionTreeClassifier.set_fit_request\\nDecisionTreeClassifier.set_params\\nDecisionTreeClassifier.set_predict_proba_request\\nDecisionTreeClassifier.set_predict_request\\nDecisionTreeClassifier.set_score_request\\nExamples using sklearn.tree.DecisionTreeClassifier\\nsklearn.tree .DecisionT reeClassifier ¬∂\\nclass  sklearn.tree. DecisionT reeClassifier (*, criterion ='gini' , splitter ='best' , max_depth =None , min_samples_split =2, min_samples_leaf =1, min_weight_fraction_leaf =0.0,\\nmax_featur es=None , random_state =None , max_leaf_nodes =None , min_impurity_decr ease=0.0, class_weight =None , ccp_alpha =0.0)[source] ¬∂\\nA decision tree classifier .\\nRead more in the User Guide .\\nParameters :\\ncriterion {‚Äúgini‚Äù, ‚Äúentropy‚Äù, ‚Äúlog_loss‚Äù}, default=‚Äùgini‚Äù\\nThe function to measure the quality of a split. Supported criteria are ‚Äúgini‚Äù for the Gini impurity and ‚Äúlog_loss‚Äù and ‚Äúentropy‚Äù both for the Shannon information gain, see\\nMathematical formulation .\\nsplitter {‚Äúbest‚Äù, ‚Äúrandom‚Äù}, default=‚Äùbest‚Äù\\nThe strategy used to choose the split at each node. Supported strategies are ‚Äúbest‚Äù to choose the best split and ‚Äúrandom‚Äù to choose the best random split.\\nmax_depth int, default=None\\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\\nmin_samples_split int or float, default=2\\nThe minimum number of samples required to split an internal node:\\nIf int, then consider min_samples_split  as the minimum number .\\nIf float, then min_samples_split  is a fraction and ceil(min_samples_split  * n_samples)  are the minimum number of samples for each split.\\nChanged in version 0.18: Added float values for fractions.\\nmin_samples_leaf int or float, default=1\\nThe minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf  training samples\\nin each of the left and right branches. This may have the ef fect of smoothing the model, especially in regression.\\nIf int, then consider min_samples_leaf  as the minimum number .\\nIf float, then min_samples_leaf  is a fraction and ceil(min_samples_leaf  * n_samples)  are the minimum number of samples for each node.\\nChanged in version 0.18: Added float values for fractions.\\nmin_weight_fraction_leaf float, default=0.0\\nThe minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not\\nprovided.\\nmax_featur esint, float or {‚Äúauto‚Äù, ‚Äúsqrt‚Äù, ‚Äúlog2‚Äù}, default=None\\nThe number of features to consider when looking for the best split:\\nIf int, then consider max_features  features at each split.\\nIf float, then max_features  is a fraction and max(1, int(max_features  * n_features_in_))  features are considered at each split.\\nIf ‚Äúsqrt‚Äù, then max_features=sqrt(n_features) .\\nIf ‚Äúlog2‚Äù, then max_features=log2(n_features) .\\nIf None, then max_features=n_features .\" metadata={'source': '/Users/snehadharne/Downloads/chatgpt 2/decision tree.pdf', 'page': 0}\n"
          ]
        }
      ],
      "source": [
        "# You MUST add your PDF to local files in this notebook (folder icon on left hand side of screen)\n",
        "\n",
        "# Simple method - Split by pages\n",
        "loader = PyPDFLoader(\"/Users/snehadharne/Downloads/chatgpt 2/decision tree.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "print(pages[0])\n",
        "\n",
        "# SKIP TO STEP 2 IF YOU'RE USING THIS METHOD\n",
        "chunks = pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iADY2CXNlNq9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "# Advanced method - Split by chunk\n",
        "\n",
        "# Step 1: Convert PDF to text\n",
        "import textract\n",
        "doc = textract.process(\"/Users/snehadharne/Downloads/chatgpt 2/decision tree.pdf\")\n",
        "\n",
        "# Step 2: Save to .txt and reopen (helps prevent issues)\n",
        "with open('/Users/snehadharne/Downloads/chatgpt 2/decision tree.txt', 'w') as f:\n",
        "    f.write(doc.decode('utf-8'))\n",
        "\n",
        "with open('/Users/snehadharne/Downloads/chatgpt 2/decision tree.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Step 3: Create function to count tokens\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "# Step 4: Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 24,\n",
        "    length_function = count_tokens,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ_gDkwep4q7",
        "outputId": "fbaa8776-a0bb-4b29-d84a-f648be4e32ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain.schema.Document"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Result is many LangChain 'Documents' around 500 tokens or less (Recursive splitter sometimes allows more tokens to retain context)\n",
        "type(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "fK31bxDOpz1l",
        "outputId": "840f9b85-e76f-4365-b4e5-cb96332e12ce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Quick data visualization to ensure chunking was successful\n",
        "\n",
        "# Create a list of token counts\n",
        "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "# Create a DataFrame from the token counts\n",
        "df = pd.DataFrame({'Token Count': token_counts})\n",
        "\n",
        "# Create a histogram of the token count distribution\n",
        "df.hist(bins=40, )\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IlznUDK-i2m"
      },
      "source": [
        "# 2. Embed text and store embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "92ObhTAKnZzQ",
        "outputId": "b5a8842f-3d75-443e-dc98-21de7c978b2e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get embedding model\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create vector database\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LPwdGDP-nPO"
      },
      "source": [
        "# 3. Setup retrieval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "RWP92zGg5Nb_",
        "outputId": "e5dcfe8c-a8ae-478f-beba-32849acf59cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"logo \\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat's new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started Tutorial What's new Glossary Development FAQ Support Related packages Roadmap Governance About us GitHub Other Versions and Download\\n\\n  Go\\n\\n Toggle Menu\\n\\nPrevUp Next\\n\\nscikit-learn 1.3.2\\nOther versions\\n\\nPlease cite us if you use the software.\\n\\nsklearn.tree.DecisionTreeClassifier\\n\\nDecisionTreeClassifier\\n\\nDecisionTreeClassifier.apply\\nDecisionTreeClassifier.cost_complexity_pruning_path\\nDecisionTreeClassifier.decision_path\\nDecisionTreeClassifier.feature_importances_\\nDecisionTreeClassifier.fit\\nDecisionTreeClassifier.get_depth\\nDecisionTreeClassifier.get_metadata_routing\\nDecisionTreeClassifier.get_n_leaves\\nDecisionTreeClassifier.get_params\\nDecisionTreeClassifier.predict\\nDecisionTreeClassifier.predict_log_proba\\nDecisionTreeClassifier.predict_proba\\nDecisionTreeClassifier.score\\nDecisionTreeClassifier.set_fit_request\\nDecisionTreeClassifier.set_params\\nDecisionTreeClassifier.set_predict_proba_request\\nDecisionTreeClassifier.set_predict_request\\nDecisionTreeClassifier.set_score_request\\n\\nExamples using sklearn.tree.DecisionTreeClassifier\\n\\nsklearn.tree.DecisionTreeClassifier¬∂\\n\\nA decision tree classifier.\\n\\nRead more in the User Guide.\\n\\nParameters:\\n\\ncriterion{‚Äúgini‚Äù, ‚Äúentropy‚Äù, ‚Äúlog_loss‚Äù}, default=‚Äùgini‚Äù\", metadata={})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check similarity search is working\n",
        "query = \"what parameters are required to run a decision tree classifier?\"\n",
        "docs = db.similarity_search(query)\n",
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1Kv_sM8G5qAo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' The parameters required to run a decision tree classifier are criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes, min_impurity_decrease, class_weight, and ccp_alpha.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create QA chain to integrate similarity search with user queries (answer query from knowledge base)\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "\n",
        "query = \"what parameters are required to run a decision tree classifier?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "YH8WwIyk5FTM"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' criterion, check_input, sample_weight, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes, min_impurity_decrease, ccp_alpha.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create QA chain to integrate similarity search with user queries (answer query from knowledge base)\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "\n",
        "query = \"what parameters are required?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXgCLzW05dmN"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "human_text = \"Build me a knn classifier for the california_housing_train ,  california_housing_test dataset with k 3, 5, 7 and print the evaluation metrics for all\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "read the text and create and print separate json objects for each model , required that will have the parameters to run the model. also ensure you include the model name in the json objects. include the names of both train and test datasets incase splitting is not required.\n",
        "\n",
        "text = ```{human_text}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpTMFDYxpxq-"
      },
      "outputs": [],
      "source": [
        "json_files = response\n",
        "prompt = f''' you will be provided with json files that will have the required details to train a machine learning model\n",
        " you will have to write the code to run them and get evaluation metrics, make sure you include .csv after each file type so it is easy to access them :)\n",
        "\n",
        " json_files = ```{json_files}```\n",
        "\n",
        " '''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUJTnwfTqSF8"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uF0DRwRG6FQC"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/snehadharne/Downloads/Copy_of_ü¶úüîó_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb Cell 21\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mhow would one evaluate a particular knn model?\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m response \u001b[39m=\u001b[39m get_completion(prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
            "\u001b[1;32m/Users/snehadharne/Downloads/Copy_of_ü¶úüîó_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_completion\u001b[39m(prompt, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         messages\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m# this is the degree of randomness of the model's output\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/Downloads/Copy_of_%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[1;32m    292\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    294\u001b[0m         files\u001b[39m=\u001b[39mfiles,\n\u001b[1;32m    295\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:606\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    604\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    607\u001b[0m         method,\n\u001b[1;32m    608\u001b[0m         abs_url,\n\u001b[1;32m    609\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    610\u001b[0m         data\u001b[39m=\u001b[39mdata,\n\u001b[1;32m    611\u001b[0m         files\u001b[39m=\u001b[39mfiles,\n\u001b[1;32m    612\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    613\u001b[0m         timeout\u001b[39m=\u001b[39mrequest_timeout \u001b[39mif\u001b[39;00m request_timeout \u001b[39melse\u001b[39;00m TIMEOUT_SECS,\n\u001b[1;32m    614\u001b[0m         proxies\u001b[39m=\u001b[39m_thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mproxies,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    617\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import openai\n",
        "import os\n",
        "prompt = f\"\"\"\n",
        "how would one evaluate a particular knn model?\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_nH1qoL-w--"
      },
      "source": [
        "# 5. Create chatbot with chat memory (OPTIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "evF7_Dyhtcaf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Create conversation chain that uses our vectordb as retriver, this also allows for chat history management\n",
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), db.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_numeric(F6):\n",
        "    if F6 == \"?\":\n",
        "        return np.nan\n",
        "    else:\n",
        "        return int(F6)\n",
        "\n",
        "def file_preprocess(filename):\n",
        "  df = pd.read_csv(filename)\n",
        "  df['F6'] = df['F6'].apply(to_numeric)\n",
        "  mean_F6 = df['F6'].mean()\n",
        "  df['F6'] = df['F6'].fillna(mean_F6)\n",
        "  return df\n",
        "def runner(json_file):\n",
        "  # Load parameters from JSON\n",
        "  with open(json_file, 'r') as file:\n",
        "      parameters = json.load(file)\n",
        "\n",
        "  model_name = parameters['model_name']\n",
        "  df = file_preprocess(parameters['filename'])\n",
        "\n",
        "  #Check for target_variable is present or not\n",
        "  target_variable = parameters.get(\"target_variable\", None)\n",
        "  if target_variable is None:\n",
        "      raise ValueError(\"Target variable not specified in the parameters.\")\n",
        "\n",
        "  X = df.drop(columns=[target_variable])\n",
        "  y = df[target_variable]\n",
        "\n",
        "  # Define default parameters for SVMClassifier\n",
        "  default_lr_parameters = {\n",
        "      \"penalty\": 'l2',\n",
        "      \"dual\": False,\n",
        "      \"tol\": 0.0001,\n",
        "      \"C\": 1.0,\n",
        "      \"fit_intercept\": True,\n",
        "      \"intercept_scaling\": 1,\n",
        "      \"class_weight\": None,\n",
        "      \"random_state\": None,\n",
        "      \"solver\": 'lbfgs',\n",
        "      \"max_iter\": 100,\n",
        "      \"multi_class\": 'auto',\n",
        "      \"verbose\": 0,\n",
        "      \"warm_start\": False,\n",
        "      \"n_jobs\": None,\n",
        "      \"l1_ratio\": None\n",
        "  }\n",
        "  default_svm_parameters = {\n",
        "    'C': 1.0,\n",
        "    'kernel': 'rbf',\n",
        "    'degree': 3,\n",
        "    'gamma': 'scale',\n",
        "    'coef0': 0.0,\n",
        "    'shrinking': True,\n",
        "    'probability': False,\n",
        "    'tol': 0.001,\n",
        "    'cache_size': 200,\n",
        "    'class_weight': None,\n",
        "    'verbose': False,\n",
        "    'max_iter': -1,\n",
        "    'decision_function_shape': 'ovr',\n",
        "    'break_ties': False,\n",
        "    'random_state': None\n",
        "  }\n",
        "  default_decision_tree_parameters = {\n",
        "    \"criterion\": \"gini\",\n",
        "    \"splitter\": \"best\",\n",
        "    \"max_depth\": None,\n",
        "    \"min_samples_split\": 2,\n",
        "    \"min_samples_leaf\": 1,\n",
        "    \"min_weight_fraction_leaf\": 0.0,\n",
        "    \"max_features\": None,\n",
        "    \"random_state\": None,\n",
        "    \"max_leaf_nodes\": None,\n",
        "    \"min_impurity_decrease\": 0.0,\n",
        "    \"class_weight\": None,\n",
        "    \"ccp_alpha\": 0.0\n",
        " }\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=parameters['split'], random_state=42)\n",
        "\n",
        "  def_param = {\n",
        "      \"decision_tree\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
        "      \"svm\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
        "      \"lr\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"LogisticRegression\"}\n",
        "  }\n",
        "\n",
        "  param = def_param[model_name][\"param_dict\"]\n",
        "  lib_name = def_param[model_name][\"lib_name\"]\n",
        "  # print(param, lib_name)\n",
        "\n",
        "  # Merge default and user-provided parameters\n",
        "  merged_parameters = {**eval(param), **parameters.get(\"param\", {})}\n",
        "  # print(merged_parameters)\n",
        "\n",
        "  # Initialize the Decision Tree model with the merged parameters\n",
        "  model = eval(lib_name)(**merged_parameters)\n",
        "\n",
        "  # Train the Decision Tree model\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  return accuracy\n",
        "  print(merged_parameters)\n",
        "  # print(param)\n",
        "  # print(eval(param))\n",
        "  # print(parameters[\"param\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "#human_text = \"Build me an for the breast-cancer-wisconsin  dataset with default max_depth = 3, splitter = random and criterion as log loss and target variable 'Class' and print the evaluation metrics for all\"\n",
        "\n",
        "def make_json(chat_history1):\n",
        "    prompt = f\"\"\"\n",
        "    read the chat history between the user and the chatbot and create a dictionary of the model parameters finalized by them. include filename and append the dataset filename with .csv extension. create  a dictionary named param (which would be the parameters of the model) and write all the parameters asked by the user in the datatype of what the function requires. \n",
        "    include 'target_variable' as mentioned in the prompt. and 'split' should be 0.2 unless some other value is specified. an example for svm model might look like this (with curly brackets instead of sqauare brackets) also make sure you write the model name compatible to the sklearn libraries:\n",
        "    [\n",
        "    \n",
        "    \"filename\" : \"breast-cancer-wisconsin.csv\",\n",
        "    \"model_name\" : \"decision_tree\",\n",
        "    \"param\": [\n",
        "        \"kernel\": \"linear\"\n",
        "    ],\n",
        "    \"target_variable\": \"Class\",\n",
        "    \"split\" : 0.2\n",
        "    ]\n",
        "    text = ```{chat_history1}```\n",
        "    \"\"\"\n",
        "    json_objects = get_completion(prompt)\n",
        "    print(json_objects)\n",
        "    data_dict = json.loads(json_objects)\n",
        "    json_file_path = \"sample.json\"\n",
        "    with open(json_file_path, 'w') as json_file:\n",
        "        json.dump(data_dict, json_file,indent=2)\n",
        "    result = runner(\"sample.json\")\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "-pHw5siewPNt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_29292/761261110.py:35: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
            "  input_box.on_submit(on_submit)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f05f6b281f7c4a3e938cce60afcced2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='', placeholder='Please enter your question:')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e547e9887964eb2aa5671f44eedc7f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what is gini index?')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0fa4a7177834896997dc3e07a2ec06c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  Gini index is a function to measure the quality of a sp‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37b7eb0d129844d6831f2050a8b7a4b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> make a model with breast-cancer-wisconsin dataset')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1edb33c99ebb49ec947cebaf3b19e35b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  You can use the DecisionTreeClassifier from sklearn.tre‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"filename\": \"breast-cancer-wisconsin.csv\",\n",
            "    \"model_name\": \"decision_tree\",\n",
            "    \"param\": {\n",
            "        \"criterion\": \"gini\",\n",
            "        \"splitter\": \"best\"\n",
            "    },\n",
            "    \"target_variable\": \"Class\",\n",
            "    \"split\": 0.2\n",
            "}\n",
            "Accuracy: 0.9642857142857143\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d123db0d0948eea534f685b1ce8396",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what is the accuracy?')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c75d6c0174f74a42b2763e6803bde9b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  The accuracy of the model made with the breast-cancer-w‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"filename\": \"breast-cancer-wisconsin.csv\",\n",
            "    \"model_name\": \"decision_tree\",\n",
            "    \"param\": {\n",
            "        \"criterion\": \"gini\",\n",
            "        \"splitter\": \"best\"\n",
            "    },\n",
            "    \"target_variable\": \"Class\",\n",
            "    \"split\": 0.2\n",
            "}\n",
            "Accuracy: 0.9571428571428572\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3248616dc7e4464a8e365c4c9240df4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what is the resulting accuracy?')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "333b865f42a14334ba0d2ae6748b9839",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  The resulting accuracy of the model made with the breas‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cabe93449e64e0a9991c76cf7842163",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what was my first question?')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec74c1c76dc84f17bad5b7e1675608b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  The Gini index is a function used to measure the qualit‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaf5036f3c414767aaa34f3b72d403ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> which dataset did I use?')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ac1de47849f4d24bbf87f857a0b394c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  The Iris dataset.')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cdb6e1309d848a98c68011120b28588",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> i think i used breast-cancer-wisconsin dataset')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9856f1f9d7e344399739169a3b26a8da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  The Iris dataset.')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04019ff0febb420894a06495ec6d9990",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what dataset did i mention in the chat?')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "790b0c4db6bf47798f61418dfea57299",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  The iris dataset.')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1811a70083a42df8e8f1181651e4b2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b>User:</b> what was my second question?')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8803e005faee428c99ee1d5486af0713",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HTML(value='<b><font color=\"blue\">Chatbot:</font></b>  Demonstration of multi-metric evaluation on cross_val_s‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "chat_history = []\n",
        "\n",
        "def on_submit(_):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Thank you for using the State of the Union chatbot!\")\n",
        "        return\n",
        "    if query.lower() == 'done':\n",
        "\n",
        "        # accuracy = [answer.split()[-1] for _, answer in chat_history if 'resulting accuracy is' in answer]\n",
        "        # if accuracy:\n",
        "        #     accuracy = accuracy[0]\n",
        "        #     #chat_history.append(('what is the resulting accuracy?', 'the resulting accuracy is ' + accuracy))\n",
        "\n",
        "        # else:\n",
        "        #     print(\"Error: Unable to extract accuracy. Make sure 'done' is used after asking about accuracy.\")\n",
        "        # return\n",
        "        human_tex = chat_history\n",
        "        eval_metrics= make_json(human_tex)\n",
        "        chat_history.append(('what is the accuracy?', f'The resulting accuracy is {eval_metrics}'))\n",
        "        # chat_history.append(('what is the resulting accuracy?', 'the resulting accuracy is '+str(eval_metrics)))\n",
        "        return\n",
        "    \n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
        "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "print(\"Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\")\n",
        "\n",
        "input_box = widgets.Text(placeholder='Please enter your question:')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('what is gini index?',\n",
              "  ' Gini index is a function to measure the quality of a split. It is used in the DecisionTreeClassifier to measure the impurity of a node.'),\n",
              " ('make a model with breast-cancer-wisconsin dataset',\n",
              "  ' You can use the DecisionTreeClassifier from sklearn.tree to make a model using the breast-cancer-wisconsin dataset. You can use the fit() method to train the model on the dataset and the predict() method to make predictions.'),\n",
              " ('what is the accuracy?', 'The resulting accuracy is 0.9642857142857143'),\n",
              " ('what is the accuracy?',\n",
              "  ' The accuracy of the model made with the breast-cancer-wisconsin dataset cannot be determined without running the model.'),\n",
              " ('what is the accuracy?', 'The resulting accuracy is 0.9571428571428572')]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_text = \"Build me an for the breast-cancer-wisconsin  dataset with default max_depth = 3, splitter = random and criterion as log loss and target variable 'Class' and print the evaluation metrics for all\"\n",
        "make_json(human_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
