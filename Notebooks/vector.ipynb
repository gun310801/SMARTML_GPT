{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"##\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "async_ai = AsyncOpenAI()\n",
    "ai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLWantReadError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anyio/streams/tls.py:108\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    109\u001b[0m \u001b[39mexcept\u001b[39;00m ssl\u001b[39m.\u001b[39mSSLWantReadError:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:921\u001b[0m, in \u001b[0;36mSSLObject.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n\u001b[1;32m    922\u001b[0m \u001b[39mreturn\u001b[39;00m v\n",
      "\u001b[0;31mSSLWantReadError\u001b[0m: The operation did not complete (read) (_ssl.c:2580)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/snehadharne/anaconda3/chatpdf/vector.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mWhat tools should I need to use to build a web service using vector embeddings for search?\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m openai_client \u001b[39m=\u001b[39m AsyncOpenAI()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m completion \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m openai_client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     messages\u001b[39m=\u001b[39m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehadharne/anaconda3/chatpdf/vector.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:1199\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   1153\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m   1154\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1198\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[1;32m   1200\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/chat/completions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1201\u001b[0m         body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1202\u001b[0m             {\n\u001b[1;32m   1203\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages,\n\u001b[1;32m   1204\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m   1205\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1206\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunction_call\u001b[39m\u001b[39m\"\u001b[39m: function_call,\n\u001b[1;32m   1207\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfunctions\u001b[39m\u001b[39m\"\u001b[39m: functions,\n\u001b[1;32m   1208\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogit_bias\u001b[39m\u001b[39m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1209\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1210\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[1;32m   1211\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpresence_penalty\u001b[39m\u001b[39m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1212\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mresponse_format\u001b[39m\u001b[39m\"\u001b[39m: response_format,\n\u001b[1;32m   1213\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m: seed,\n\u001b[1;32m   1214\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[1;32m   1215\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream,\n\u001b[1;32m   1216\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[1;32m   1217\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtool_choice\u001b[39m\u001b[39m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1218\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtools\u001b[39m\u001b[39m\"\u001b[39m: tools,\n\u001b[1;32m   1219\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m   1220\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: user,\n\u001b[1;32m   1221\u001b[0m             },\n\u001b[1;32m   1222\u001b[0m             completion_create_params\u001b[39m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1223\u001b[0m         ),\n\u001b[1;32m   1224\u001b[0m         options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[1;32m   1225\u001b[0m             extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m   1226\u001b[0m         ),\n\u001b[1;32m   1227\u001b[0m         cast_to\u001b[39m=\u001b[39mChatCompletion,\n\u001b[1;32m   1228\u001b[0m         stream\u001b[39m=\u001b[39mstream \u001b[39mor\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m         stream_cls\u001b[39m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1230\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1474\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1462\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_AsyncStreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1470\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1471\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1472\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39m\u001b[39mawait\u001b[39;00m async_to_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1473\u001b[0m     )\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1275\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     remaining_retries: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m   1276\u001b[0m         cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1277\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1278\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1279\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1280\u001b[0m         remaining_retries\u001b[39m=\u001b[39mremaining_retries,\n\u001b[1;32m   1281\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1299\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m   1298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1299\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, auth\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_auth, stream\u001b[39m=\u001b[39mstream)\n\u001b[1;32m   1300\u001b[0m     log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m   1301\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mHTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl, response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mreason_phrase\n\u001b[1;32m   1302\u001b[0m     )\n\u001b[1;32m   1303\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1617\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1609\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m   1610\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m   1611\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m   1612\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m   1613\u001b[0m )\n\u001b[1;32m   1615\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1617\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1618\u001b[0m     request,\n\u001b[1;32m   1619\u001b[0m     auth\u001b[39m=\u001b[39mauth,\n\u001b[1;32m   1620\u001b[0m     follow_redirects\u001b[39m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1621\u001b[0m     history\u001b[39m=\u001b[39m[],\n\u001b[1;32m   1622\u001b[0m )\n\u001b[1;32m   1623\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1624\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1645\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1642\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m auth_flow\u001b[39m.\u001b[39m\u001b[39m__anext__\u001b[39m()\n\u001b[1;32m   1644\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1645\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1646\u001b[0m         request,\n\u001b[1;32m   1647\u001b[0m         follow_redirects\u001b[39m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1648\u001b[0m         history\u001b[39m=\u001b[39mhistory,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1650\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1651\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1682\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1680\u001b[0m     \u001b[39mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1682\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1683\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1719\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1715\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1719\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m transport\u001b[39m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1721\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1722\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:366\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    353\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    354\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    355\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    364\u001b[0m )\n\u001b[1;32m    365\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 366\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\u001b[39m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mAsyncIterable)\n\u001b[1;32m    370\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    371\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    372\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    373\u001b[0m     stream\u001b[39m=\u001b[39mAsyncResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    374\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    375\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:268\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:251\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m connection\u001b[39m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_async/connection.py:103\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_async/http11.py:133\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_async/http11.py:111\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_receive_response_headers(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_async/http11.py:176\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_receive_event(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_async/http11.py:212\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_stream\u001b[39m.\u001b[39mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_backends/anyio.py:34\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mwith\u001b[39;00m anyio\u001b[39m.\u001b[39mfail_after(timeout):\n\u001b[1;32m     33\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream\u001b[39m.\u001b[39mreceive(max_bytes\u001b[39m=\u001b[39mmax_bytes)\n\u001b[1;32m     35\u001b[0m     \u001b[39mexcept\u001b[39;00m anyio\u001b[39m.\u001b[39mEndOfStream:  \u001b[39m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anyio/streams/tls.py:171\u001b[0m, in \u001b[0;36mTLSStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mreceive\u001b[39m(\u001b[39mself\u001b[39m, max_bytes: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m65536\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbytes\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_sslobject_method(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ssl_object\u001b[39m.\u001b[39mread, max_bytes)\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    173\u001b[0m         \u001b[39mraise\u001b[39;00m EndOfStream\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anyio/streams/tls.py:115\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_bio\u001b[39m.\u001b[39mpending:\n\u001b[1;32m    113\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransport_stream\u001b[39m.\u001b[39msend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_bio\u001b[39m.\u001b[39mread())\n\u001b[0;32m--> 115\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransport_stream\u001b[39m.\u001b[39mreceive()\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m EndOfStream:\n\u001b[1;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_bio\u001b[39m.\u001b[39mwrite_eof()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:1105\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[0;34m(self, max_bytes)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_protocol\u001b[39m.\u001b[39mread_event\u001b[39m.\u001b[39mis_set() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport\u001b[39m.\u001b[39mis_closing():\n\u001b[1;32m   1104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport\u001b[39m.\u001b[39mresume_reading()\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_protocol\u001b[39m.\u001b[39mread_event\u001b[39m.\u001b[39mwait()\n\u001b[1;32m   1106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport\u001b[39m.\u001b[39mpause_reading()\n\u001b[1;32m   1108\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/locks.py:213\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_waiters\u001b[39m.\u001b[39mappend(fut)\n\u001b[1;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mawait\u001b[39;00m fut\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "prompt = \"\"\"\n",
    "What tools should I need to use to build a web service using vector embeddings for search?\n",
    "\"\"\"\n",
    "openai_client = AsyncOpenAI()\n",
    "\n",
    "completion = await openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sklearn.tree.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "async def completion1(prompt):\n",
    "    completion = await async_ai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages= prompt,\n",
    "    temperature = 0)\n",
    "    return(completion.choices[0].message.content) \n",
    "\n",
    "\n",
    "    \n",
    "async def completion_from_messages(messages):\n",
    "    completion = await async_ai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages= messages,\n",
    "    temperature = 0)\n",
    "    print(completion.choices[0].message.content)\n",
    "    return(completion.choices[0].message.content) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "messages.append(\n",
    "{'role':'user', 'content':'what parameters i could set for decision tree. '},    \n",
    ")\n",
    "response = await completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5772dee11e7542f19556cb1fc3782cf1',\n",
       " '6e295bc38e574ef09af0825dc27f262b',\n",
       " '33a3fad8ceea4dfb9ad5c322d1ec25dd',\n",
       " 'a736977a964443be8235089739250f4b',\n",
       " 'bc82ca85c35d4d7fb3ddb0877a9fa11f',\n",
       " '2f826b23d98c4104a9dd813c1ce00562',\n",
       " 'a171aca59fc94ddfa1ab118fb67e5f25',\n",
       " 'aa3f3394430c4b8193b6e4cb69a34786',\n",
       " '84bb1ec2ae1842198c55fad54c5839c7',\n",
       " '0478cfb381a448338dd51dfce5082078',\n",
       " '9284f952a1154475b1074cb3ef10d70b',\n",
       " 'd0c9e914b77d439a92645030bf6103ad',\n",
       " '5d2f237d483c498aa48ace0d9de21af7',\n",
       " '19df959b4cb64494b4f4debb266abc28',\n",
       " 'dcffd7dfdfdb4964a7ccdeb1fe6a549a',\n",
       " '1931b569a160415b913cca453ac2d512',\n",
       " 'fcd8c27202124a5f812f8fd752a62571',\n",
       " '3ea3ca6e41ee4c909ea96ff0b3c3cfae',\n",
       " '1e6abcc625e64c8199343ee671982b8a',\n",
       " 'c4ecfdd7a18645298f86677e4e8e3d2c',\n",
       " '1e67fb9fbd334043b531028ee18c2bbc',\n",
       " 'b8f4ca60a6b34712b5ea1bd3a8463768',\n",
       " 'b5183e840dab49dcac0f138cf73b9c97',\n",
       " '84167030090f46b086bfb41985d9a8e0',\n",
       " '19e74879d7cf4229ae0f8a349c113cdd',\n",
       " '3f5d033cc12741dbbcca3a866a15ec80',\n",
       " '67e7950dc3754f9592984571c4710ba3',\n",
       " '4a4e1fd9569049c8ada27b4a974f0b14',\n",
       " '9651a933ece94d3b860188fcee60fd1b',\n",
       " '5288acbfe5f54f3aa3d834ccae55f9ba',\n",
       " '4b56346b00454f9d879f9432898b05db',\n",
       " 'c036170c50264ad3a79395f3609638a6',\n",
       " '991cbe9d464f45dab5cee37e1c1e441c',\n",
       " '2fe8534fd5304af99d1218c260709f73',\n",
       " '263c4677d8344704b9b6acf74dff1591',\n",
       " '435cd2930fb6486b9f4373e10a9c79f0',\n",
       " 'c066338f349945529a3a63e9e1bbfcdd',\n",
       " 'd88cc6645b1c4422ae008d83b632dcf6',\n",
       " '35b131b8b9ca4f94b01d3273c7f459ef',\n",
       " '1cf7f64e463c4e5092ebda34807e8aa8',\n",
       " '386bd69c6d824e3ca90f7a599bf51026',\n",
       " 'f5c73da1b1b2461480b07982a88403c3',\n",
       " '34843bf5e5a2402180a94b4992bd97c0',\n",
       " '78141a124ffd413a89b3e640ec973d71',\n",
       " '25942b4e5ec8422cbaacd34c64649e0d',\n",
       " '4a114024e5db4a64a649a49af1a6c065',\n",
       " '3f0dc9df9ad04944bc1614f93ed9e1fb',\n",
       " '5a933322b06c4e71bc0ffe633f3b1191',\n",
       " '63cd127de8514818b90d351c637760e8',\n",
       " '90112f9af72544fc8d149ff392bbd509',\n",
       " 'e481b50946a54a1fa6f136e5ed461045',\n",
       " '2ac6447e82b14b59beaf8c1798796172',\n",
       " '6fec389f21ad4e9f97aa12d394886b6a',\n",
       " '5776f1d7007f4d55a18c2e4f750a2984',\n",
       " '75b08b9b49fa40c889264e2794c2ec51',\n",
       " '736c2db475a744ed928320bb98a44b49',\n",
       " '8172c34a247d4405b09b13bb4997d595',\n",
       " '617e2a9f9bb245a388cda063504f2bf1',\n",
       " '871bcd893b48439188634dcb0acb105a',\n",
       " '2f646d9007194ecf9851e172d326a505',\n",
       " '01a6a388cb3e43e18e9e7cebb711cb03',\n",
       " '04fca88645504f80a3ef2347da51033b',\n",
       " 'fb3027b966c84cbeb63b9f60e694ac11',\n",
       " 'd58fba3292c24d43a489ddab1b003654',\n",
       " '79cbfa4e05a74455965a3a8878afa954',\n",
       " 'eb71dd5e23b24d27b37cfc5034cfb5cc',\n",
       " 'b3c8f2d682d84778baf28f3ff72900d1',\n",
       " '461c0d84c8b44a63b4c37ae9d37cba5e',\n",
       " '8c307b1d9bfd4191ab4198a057a1fbb6',\n",
       " 'c5ef24288daa41128ae4468d694e216a',\n",
       " 'ced799d042fb4d658d48b7905ac297b3',\n",
       " 'b796f11f423742a0ac9195999bc684b3',\n",
       " '02da9c28b7b54c4ea824dcbc18f7ee6b',\n",
       " '031b6442d07b41f38f4a8198fe35ff34',\n",
       " '3e78c97521bf45ccab4128e88e3b45dd',\n",
       " 'c1c530b274c94c849f94fba6f0b1aec2',\n",
       " 'b624be03d0e14b11b9f9fdfbfdf3fffe',\n",
       " 'd9f3eceff5b746dd98b1c864502d214b',\n",
       " '8a286ddd05c44d55972dc067b3ce4c9b',\n",
       " 'a6483eb5a6ce4c1fafe08ced6968bb5f',\n",
       " '8aa6f02ba38444d0b44efecbcdd5da8d',\n",
       " 'b92237378ca54a418c77f6ff5bfc3b64',\n",
       " '63bffb61c6424f7584e850bcd4c28a71',\n",
       " '9ab2bc1c7feb464ca96df064b001b795',\n",
       " 'f3d85ac0d9ff4f6ab0f9f151f5ee6000',\n",
       " 'f105c65b702a4fb3b842c481a62273c3',\n",
       " '5ef36cf3db4a45f196aa7ad522658136',\n",
       " '6042972b43bc4eaca2a13d7fcd5eda18',\n",
       " '8a6f041986db4570817ba4c9f53c6bbc',\n",
       " '25807ccdcccf466397e02f84f6732f19',\n",
       " 'ccc1c575b333446984b8c0f4ef147642',\n",
       " '37f7e43f9dd84d3c93fbc457c79f60b4',\n",
       " '18576b035711437e809a3654d0d176da',\n",
       " 'c90d516c4b794a0cba25dc34a0ff7dc3',\n",
       " '9e84a8cd27f340579c50c38c046a0544',\n",
       " '725c09d6925d45e0887b7bdb6ac44e8c',\n",
       " '036b34958481492ea659d5a1d87bbd14',\n",
       " '955ce8ec71704f828ad4aac38300a95f',\n",
       " '9d393a7e2e9a4912a18979ea619159bc',\n",
       " 'cbf6a66be8a84f79a54874f6cfa8efa6',\n",
       " '3dc748efe182482db77018a3e6813ffd',\n",
       " '099dce6ec96147dab1cdd9f674512874',\n",
       " 'dbf4f83a58734fa2b7ff2f5ac825fafc',\n",
       " '2c5922315f114411a50527ec3ba5e7fc',\n",
       " '4affc17fcc634ae08d674f016b6caa08',\n",
       " 'e900d9cee2074571a037ec46e3515960',\n",
       " 'cece16f36b714028afc772345fa7f088',\n",
       " '2cdf93575f2d4dceb54bdddc4507a532',\n",
       " '0f2357b9037d4feeaf38933c98c51d42',\n",
       " '6bca1ef6f736475698405b47a0f00dbf',\n",
       " 'db77c7511c9f461d873d3fb4f73cbbe6',\n",
       " '0fe4bc82951d4fe28d4fe3dda6ccad48',\n",
       " 'a032e00d685f4f4faf3764cf0c1b66c6',\n",
       " 'cdadc7475dca41e7977d070f637c79c6',\n",
       " '1032802cb50d40dcabc6c8b57d489748',\n",
       " '2088034d57cf4aa7b69767cb7c710d4e',\n",
       " 'f40d9cf7278d489a801d5043fadbf67b',\n",
       " '55b02a1906a74f889a30e4e879c6ae39',\n",
       " '3946d99aa8b94611bfdf948fb4f3e8b6',\n",
       " '5dde01f609b547d09fbec8225c5bf27d',\n",
       " 'e30c93e77bec46509f704986350e062a',\n",
       " '438e7c41f7374f2489e2f7665e12481e',\n",
       " 'bda9cf49fe804631b25bf4514ec664ef',\n",
       " '56707be97b324329a0ebb0d092d23012',\n",
       " '6772da4185714a2394e667201322dc5a',\n",
       " 'd2b508899f3e4dafae08dc7ce0f54fa2',\n",
       " 'ba252110d68d4a07800c097fb3af35a4',\n",
       " '87afa934bdc34a568b08aa8c309e9002',\n",
       " '5aa11c40358740d8aa4d410b1795822a',\n",
       " '97aebf7143d442e4bb36e6c165b06449',\n",
       " '3e0868c67d65489f9584f1e534eba364',\n",
       " '58ef6df5b80a429eb5f966aa9d86cdcb',\n",
       " '805e5e9028784736af7ab89fd662a5e7',\n",
       " 'f9e96491f80d45228ab658ddf8f14273',\n",
       " '3b19d71fc03e45779fffcbc79741a844',\n",
       " '36b27ea24ff940f6babf1bf20a1a3176',\n",
       " '2760d97b2469487d956b21b4283787ce',\n",
       " 'b4f5a773a5ea456e98867233a661f72c',\n",
       " '2dd2d6ff3885464ebb93ae5c0dedba64',\n",
       " 'd3472a1d5d5448c58807bf4388672371',\n",
       " '3579471e81834a15baed0847f5750e86',\n",
       " '338242faeea744a8b0e1284925529434',\n",
       " '56abc7fbbc6548e2a9a23805ba3a957b',\n",
       " '2456528e051e4469ba0980d1af73f703',\n",
       " '70e94d6ad62f44dba824c3d7789ff653',\n",
       " '46f89686f2a9414cbe25b362a81bc988',\n",
       " 'ab7e44242e774089a66c167775064916',\n",
       " 'd217e18a01954d5cb4a1928b27476595',\n",
       " '1a2d4f5530ea47c8a710d0a95d35639f',\n",
       " '3e3500bb0033436ca78732c5bee7d30d',\n",
       " '0b48d22089704f5db9e68ced1cfbf080',\n",
       " '6a501952eabb4b2f83a7f15e0119bb04',\n",
       " 'afcc296e8e2a4bf28e888e7330981b1d',\n",
       " '71a54c88a8c345cba0ac37758d1443e2',\n",
       " 'ad09ee96e67647e2b4c40204774dd5ae',\n",
       " 'fc13e8cac2ee4ec7b5721106c3d52f1c',\n",
       " '4694d4356c6a427ba36872d395a768e0',\n",
       " 'e94e470266904f8fbc614ec5911584b8',\n",
       " '17e97df8c31b4220a94c055c95e014f1',\n",
       " 'd8a439669df14f74b03382ee1f3da6b8',\n",
       " '745c7aaceaea406f9e156e5a1a2af189',\n",
       " 'ea8dec1a0fa6424e98bd691bc66cd321',\n",
       " '7a263cc2e7ab4b6b81e26a2ea9e5177b',\n",
       " '997e504d07da4e05b00307394d36bc77',\n",
       " '8a14fccd26324f068c8bc65f4e635479',\n",
       " '3397deace22f43a4850265aa522c48e2',\n",
       " '55478e7c54f54515b4a9086799679f70',\n",
       " '9b3e7e19f80e48c89d28def20f57f2d2',\n",
       " '8732242fe835433a87ea10f5dd27524e',\n",
       " '517170133edd49bca766148daa9c6fa5',\n",
       " 'df31b4cf7e374a05af5005881b08931e',\n",
       " 'b3e33721df594d6baef8114d2ed8c58d',\n",
       " '94b58f30162e4d909325bdfb091e3bbc',\n",
       " '5fd54b06bb3843c99c084c136a4fcbf4',\n",
       " '9d8ee39159394e3fa02555054e3253cb',\n",
       " 'e57033359fc7453e8a4d2cd46efc4961',\n",
       " '77650d2ba0db4a4ba18f1480d84c500c',\n",
       " 'f905f23fc64b4ee3b2c16d5ad91df967',\n",
       " '37189f3d3e6b43b9b42f94a82236da95',\n",
       " '4ed5a75498f64e8cb39bc6b237d77dba',\n",
       " 'a9ad13574fea4e5cb3d03f0543577c4d',\n",
       " '8ab2c8fed208455e8f6226cfd38b888c',\n",
       " '4d7836cd3409430c8ea5513955ec789e',\n",
       " '884d338b395442479201380214301a3b',\n",
       " '878b4da47fe541388656def25464ef04',\n",
       " 'f477a034a2d24551a3c525c3adaa8836',\n",
       " 'b99ab25657b14d8c821b09e378f8fd0d',\n",
       " 'd842fac470674f129eb61deaf1f55452',\n",
       " 'dd8fd8cad6ef416ab801b671509736ca',\n",
       " 'a1689557117443acb0789ff8b1e8b92c',\n",
       " '67d0863b8a7f4c04a838f63813020f57',\n",
       " 'c4074ae719504300af9b2f07bc887b74',\n",
       " 'f546219578d1463d8135db8e27814ffd',\n",
       " '785c13cbf4524c4bbb022d441a3828a8',\n",
       " 'ccdd1db7b4d347ce84f5ff7d7ae8c138',\n",
       " 'abff60f95e784d978a653c53b6746220',\n",
       " '9d09435ab3f24aeaa8c948c85af5272b',\n",
       " '22e0405f69c04fe388f37a7f611d8d8f',\n",
       " '604718974971473b9783e4c4f298c0c3',\n",
       " '3dff22bc199f43b19fabf5e123d9bd5c',\n",
       " 'f9bac581e31a4c82bd0dd3d77c0a9a37',\n",
       " '2b510bec8bd34c76a5596c03ec5d17b9',\n",
       " 'b76f3c23bff246d4b8b1153e9e9fd297',\n",
       " 'c08295c2ade04067938956d50857d858',\n",
       " '1390c25f4e364927b4f993370b5bda27',\n",
       " 'caf4f3c75ca04cf9b38bf069e1b9d8c8',\n",
       " '319a9e9c0baa4371a9a3104657696c10',\n",
       " '448289bf080c41e5b096093f4395f5fe',\n",
       " 'e0f21caf04b04c60b46307bb9ae2d05b',\n",
       " 'acc0fdbd84c7429b926fecb35b911a5e',\n",
       " 'f3726d0590644de4b92881c6ae913da4',\n",
       " '9656feab9b1845cca3e3e6e768d72c26',\n",
       " 'e103d69e1caa41caa34632d21ef08e02',\n",
       " '7f0155c061de49b38955d81a7c0a0f20',\n",
       " '1bd91e334d7546fe8901367594f37c83',\n",
       " 'ac59e6b304934c30b85628e35a767d83',\n",
       " '4efaaefbf8294c6ab54c88f0b81f02c4',\n",
       " '4f00b1ec2e004682b75e8fd296f47f8c',\n",
       " 'a8949267d273406397245b52d53d5814',\n",
       " 'c09c9a6936984671800b84fc2290cd1b',\n",
       " '75801118c50945ea8875a911abf2bbfd',\n",
       " '29ea37f25da647dfb4df59afc69cdcdf',\n",
       " '16acd8cfe21e4143b2b945c8510ac276',\n",
       " '3b04a31519df44a09de3b265693390af',\n",
       " '225a5d75993e45098e594cb9deddb74b',\n",
       " 'f0c56152d94c49468a8c61afcb448cff',\n",
       " '0bdbdca9dd204825a5b96fac4f0e755f',\n",
       " 'ae40d70676a94006b39f51cfc34ce64f',\n",
       " 'df2d893cf61d4908bc0cbf93fad08511',\n",
       " '423a02493ab94db7b784b335994fb54f',\n",
       " '8d63e140f3814dd4a975834e06050f7b',\n",
       " 'afc4f2fc873a40dca67d1e404eaea6d5',\n",
       " '6bf6ee3ab7c64fa2a4f5a0f1cb99758e',\n",
       " '83fc26c7f2e24796b6834c065429343d',\n",
       " '8a7e3fe41ce240ff9850c2d05fd32f99',\n",
       " 'c80789b8598a4f649bb635de202275ab',\n",
       " '2f71afc0d96f4a78a93b62ea272f63c9',\n",
       " '924e06b76c024f66aa0e047431d27858',\n",
       " '082dafcdb9aa4c5395e344072b0649b7',\n",
       " 'da6668e54e324d19b4ef333a631bbc6e',\n",
       " '50fbdd5ff0b741f0881c65fe44d24e84',\n",
       " 'd5c2ed5d902b4158b1ac2992e87b664e',\n",
       " '157d7ffea134497488b6e1c208aa04b0',\n",
       " '1ecbc21619554cc9b26b16dca15d7e8b',\n",
       " 'dafef2725f5e44ed8e4e7e41c3ba0d41',\n",
       " 'be8c501f80c6404c97ad8316c8dbd485',\n",
       " 'cfbcf81975054a6197c50acfc5c2eb03',\n",
       " '70de3b64562e4c1da0e0c7efdf0ef390',\n",
       " '03f0b42aaa114b30bb2bc7b903e89bea',\n",
       " '7b474061a0f04d548676413690b032c0',\n",
       " '6bed080dbe514156bbe080ffe28fd683',\n",
       " '97008675b59743ba84c2491d755e7348',\n",
       " 'ed2f8017ebb5496998c18ef3d9ad69a0',\n",
       " 'ad0c070e4dab4231ad3ab2fb354c2bd6',\n",
       " 'c6bb5877acf14a1d96c6363803e27904',\n",
       " '35edb98db0ac46b4932909297891b924',\n",
       " '3bbb20c5e91f4d2ea63003af652cfaa4',\n",
       " '849cd78f7f7442d3a076a8e8c10b0f55',\n",
       " 'e2b4eac5e69d494b8355090a4b5e8cd1',\n",
       " '8fa9ddf7286d424896d928ee003a383b',\n",
       " '33a09ba5efcc4679afc3e21a7cc8287d',\n",
       " '9a8e3d3113b44b50831fc185a089b21d',\n",
       " 'a2ebbff4dc2148aaaa9a028a352e80aa',\n",
       " 'fb283fd90dd14828a898541666f92782',\n",
       " 'ae6c6f95824c4139bec1b9df58c000f9',\n",
       " 'e3ee17e89ac9473986852d59179aa7e0',\n",
       " 'd3c1366c651043b08bd0fe10f906d011',\n",
       " '263a2e4400b94382816dc9891f9f50b9',\n",
       " 'a6317c35bad545fe827b265269ab5e02',\n",
       " 'd2e0abe8e3564acea2c5ab2057e87e21',\n",
       " '3850b53ebbef435cb9cf1af5417e1411',\n",
       " 'c94f794728d249f18ed4faea78d326bf',\n",
       " 'c93326fb842349dea1d903d156be8f74',\n",
       " 'f174e02fb3d04cec8c94309d55d4a93a',\n",
       " 'd093ed844df24de68fac6b4dd8a2502c',\n",
       " 'f1179934239c4e20910e77c7889f06be',\n",
       " '381c12e59c9d45c8a2f0656523f890d1',\n",
       " 'cc1cdc99deb748f08bff7895f3d95fa8',\n",
       " 'ed4844b4be6b47df9bbe2cddc76a8302',\n",
       " '1dd16d92508049a29f918aa98390a5d7',\n",
       " '52668bfdcf4a494fab4f459d6c3b4283',\n",
       " '68b5845d51e640d38eb0f33ea0c1644d',\n",
       " 'e0ffe29ae433423a8c25dc0d4a4e96e6',\n",
       " '4d3d53a6bcb64ecbad27073dcb83466e',\n",
       " 'f6d7336d0b3646608c55c737f1693d75',\n",
       " 'c9dfd297bf3c4b339b60377f0026c1a7',\n",
       " 'ce47819f32a14d14b0f3436eaa0d4dec',\n",
       " 'd7164f675b404df184a34247acefe79e',\n",
       " 'ba585742a17a48bf867e97ed157f2ce2',\n",
       " '71f7f6e12cde443791a9d4b6f1e8dd60',\n",
       " 'e6abc5de78c94cc8ada657365f390412',\n",
       " '350df1f6bf534f5b94b59718a696f5e5',\n",
       " '72771f01628f4865be3449ad6a304275',\n",
       " 'cf3a0d5fdba84734ade0718927199534',\n",
       " '33ee9c410ff74034a888cce8847b8419',\n",
       " 'bef0c31a4d8e4ed8b625f79142cb69ab',\n",
       " '0ef2254433304b40a7b2ede532eb61fa',\n",
       " '63ebd5d3bca04749aee8ca6ac32d89ff',\n",
       " '7343c5bc655f476b950580dc5bf13744',\n",
       " 'f6177120d5a949aab3314476be94f977',\n",
       " '549e86b9ee584890a147440ef358cef7',\n",
       " '5cb2c5966c8141f590332d4053508e83',\n",
       " '8a99c49568174bfeb2b5c4f1d80d9f81',\n",
       " '595a1b2076bc425182abbe5afe04a1a5',\n",
       " '10dcd3a505c1446bbdc4888e4571419f',\n",
       " '29391ad813724348925527f4d2d9608e',\n",
       " '1d760c9259934d5e89480ccc41bd9426',\n",
       " 'ea198d9301734aaab97fbf59fd538ea2',\n",
       " '3d87869894df4484942749708dc732bd',\n",
       " 'c87149f6f2dc4dc6828a9ece19c77204',\n",
       " 'ef7569d09e5b4390b979c17b7a051500',\n",
       " '249ed49db63b45328817177daa9020cd',\n",
       " '053d8490293744ee913321682e8b8cd2',\n",
       " '0896aebdf3754678b30fc6e027f41803',\n",
       " '4438475c058646b6869abec4d551bbc6',\n",
       " 'c02b12576c1d41d19cf1f170033be3f2',\n",
       " '587a76da84f749b79e3126ebcc98e775',\n",
       " '3934a34fee9d4f55911ff4720213f2ae',\n",
       " '633dff5ad23c433db5fac9f617a8dfda',\n",
       " '5db01fb0c9f94c8ca3bc9b7c3b1c22f4',\n",
       " 'd918afeaa7e44d8f8e9f452e3f27a36f',\n",
       " '8cb6985e551d4b5fb69bda79033c4467',\n",
       " '045d80a856ab49599a59c38cf1bc7f67',\n",
       " 'ab9cb25156da40fab1d8675304caeb68',\n",
       " '58329805a4194c229b56d6425763ba80',\n",
       " '64ff168a1deb4d4b87ab301d4b168f60',\n",
       " '99f4e9e3d3c040d1a8b6b1ebcbfa9ee7',\n",
       " '49c94bb8857f4c229ab8a8a8dc165ce4',\n",
       " '6b2b5f1d98ae41288b96fe40be710567',\n",
       " '273d6369054e4a11b1b3723203d798c0',\n",
       " '06d69ba35f6947a8b64e7ae67469c614',\n",
       " '10296c0a5b03437ebff233def79b6ffe',\n",
       " '337e54085f104eaca1a573ae466fef2c',\n",
       " '2acf66682c504dcba4b42c07322a28a4',\n",
       " '156b186322654509aecfb37505716de2',\n",
       " 'bff843f6c62a4035add7ee49e414a46e',\n",
       " 'cb044be2d7ba4d90954bd9b3c0c1d1de',\n",
       " '9ea34bc63be94ad69c02afc14298fb85',\n",
       " '22536c508e6a4b22b0026a70e418f860',\n",
       " 'dff342ce759d434a8c01d5a3a6e97095',\n",
       " '96561f444d7348abb09be7e7e978c632',\n",
       " 'a4d6ed75078c4f02a04374ba8463654d',\n",
       " '7bd58383a823480a867ad7c069691cbc',\n",
       " 'c876e0b55ac54636ad4565836262db1a',\n",
       " 'dc05154e89da471999228ecb4eb686c3',\n",
       " '9c6d74d0f9ac49e5991cad957886c21c',\n",
       " 'd173946b2ffe44afa9830713076147a4',\n",
       " '21955f30239a48eab4feca4c22bfd2aa',\n",
       " 'b1977dfa05fd4244a3860d9fa796069a',\n",
       " '902e44c67bfb481ab669acb0136a9161',\n",
       " 'fa0edb104aa543e59555fc606854811d',\n",
       " '90f17ae1fc9e4d06bbf8068907e967c0',\n",
       " '7d0212b85e8243b7b4aac99305223b49',\n",
       " 'c279d9b95e9644ad8c3bcb87333aa834',\n",
       " 'c883ddb470884028af72e23e6b3824f0',\n",
       " '20f5150071614890880f227bf288466b',\n",
       " 'e4f74f485b2a42b392f3eb87085af926',\n",
       " '6420fc2f30af4dc586d73a62f9cea1ea',\n",
       " '17826e0195754e66bd6b14829e813a60',\n",
       " '9dea2d9af4904d508b8bd9686785ac0d',\n",
       " '111206b4b9854aadae10d03dc6ab6a55',\n",
       " '6bcd8dbb380b4e2fa020eb7927fac08a',\n",
       " '7f97c283babc48d38a5b15cd184b0f69',\n",
       " 'cc0f6626ece04d7890bb757bacc5e8e4',\n",
       " '20b0ed08e7b4486abac58990608bc1cc',\n",
       " 'c9b70d0efe294babbe0a0c9f6a6b6817',\n",
       " '727ae3b01eac4f6c90008cbb175babdc',\n",
       " 'ec133931f55e468ca355249286cac0ad',\n",
       " '553a754055954a4781872b490dbd64bc',\n",
       " '610915a253e24cc2b989d8102940ffbe',\n",
       " 'f1554be8d3e143c4b04386699ff991c8',\n",
       " 'c75dbbb416f1422b9c7e2cdba75bc8c8',\n",
       " '01a12794df55456a9ca3300463ee1891',\n",
       " '44403ccabcca45778e9308cfad2db993',\n",
       " '6def852293d249649550bfc0fc57b013',\n",
       " '08dac73d80d54ba6b40ec3b18746a91a',\n",
       " '5c7e6d0742bc4f119469e03be148a76e',\n",
       " '2cb8cbb072ee4997aa2fd9f33494a678',\n",
       " '0fc4814268104c728ca031276fc38171',\n",
       " '0b79811237cb43cf8ede826f788b54d5',\n",
       " '1c487212ce66434fb00e1ac4aad0516a',\n",
       " '532da535909c4a1593129f52c75965bd',\n",
       " 'c7f7087246b34d2cb06d75a48d14a30c',\n",
       " 'a37016b6cc1a4b9280333b5e6f4f8c92',\n",
       " '973b22900c8d4ad985040ee58bfd1d15',\n",
       " 'f0a7fba4e9454c998bf6c06181f0b3ca',\n",
       " '5e87253fe1b0447391252f5f9a19c787',\n",
       " '39ffa3e58bb544269d0e200d11b9f7ca',\n",
       " '3d474270a1de40b88431c6d74ac840d7',\n",
       " '54adb42f370e43edafe5bb084939bff6',\n",
       " '3313731d7876438aa49fc067bceac01d',\n",
       " 'a8014afc891b4585b95de0b5aeb6946f',\n",
       " '835980220c9e48f1a140e6767fd59b20',\n",
       " '7b384bd9256c44bc9ec89255ebc08d77',\n",
       " '4be29012efce46a692ab045119a8b547',\n",
       " 'fb30b95df68746dfb536c869dc715f09',\n",
       " '9c9cba6c028e4293ac0c6a218b22cd8b',\n",
       " 'c4475622ef8441ed9cf1200a4d187f03',\n",
       " '72e60e0bc5844079a6ab382525e47f52',\n",
       " 'aeec00ccfd8343bc84812983f20f00c6',\n",
       " '5f2b93393a7b4bd18d966339520efb08',\n",
       " '7ee9847a05c5457b923d5691ab53bfce',\n",
       " '21d5707fad3649729a8c8d4884d9ab75',\n",
       " 'd8329e61bbac4c3695a2aa2289de92f2',\n",
       " '48cefd6a66154d0c97fbf9bb0c9f426e',\n",
       " 'd201aafeb82b486f9f786bd037d842f0',\n",
       " 'f8897edd97304288832380d500de3679',\n",
       " 'b3eb8343f19f47849575a15a16a9c2f1',\n",
       " '9b8719f88c0b4c7fb2443c969aa6f888',\n",
       " '9ae54589bcd14594b474afc3093481d9',\n",
       " '3ae9d70a834645a2a346dcf28b6a1039',\n",
       " '80deb91d240e4bc8b582042aab54f7a8',\n",
       " '9b709899d829443496a069538f70295d',\n",
       " '3460248a3b914b43b3f180277b4e31e6',\n",
       " '8cfca234fa904a1ba751e0c914759853',\n",
       " 'fd12bb7feb5a442fb9e97770fcc86419',\n",
       " '35302a6062854857b94d8583e0062f70',\n",
       " '9b4abecccd7149f39fc393be90ed5308',\n",
       " '0e1c498a415e46759476d6f94d9df643',\n",
       " 'b202285a60cf4f1b9ec41d2c7a01db49',\n",
       " '399ebdda0b72486e9644fcf2e8173990',\n",
       " 'cc322ce8f81b4ec1b1842dc6a4801cca',\n",
       " '54d932aedb1646a0b1b315c42bc18c1f',\n",
       " '578871078dc6456f84271165f5aeaafa',\n",
       " 'cee6b427505e45f49ccbb5ceefb6182b',\n",
       " 'd48fd845a5cf4b76b7e373c6b6b63cbf',\n",
       " '272ba052efb5483fba29a07565d0b3bc',\n",
       " 'e06596f2b387407f815d3b021bc9b283',\n",
       " '757edb04e92f4da3a3235abdb8c89f38',\n",
       " 'f06bf5c242d740b88f5c180905894c14',\n",
       " '30c132ba086d42debec82cf8ffe1c21a',\n",
       " 'c63f5d05d5a2480faa803df5310817c0',\n",
       " 'bbef04ea3ea5415c8bebd2347a32d0c2',\n",
       " 'f16f9a3b9e4245ac9d64cb6a69b910d7',\n",
       " 'be2f189be1eb4b968a73dfd58a5eb7cb',\n",
       " '800ebda5903e4e36a4390175dd68b67f',\n",
       " 'e53fced98c4f4cb985d6bf5dbb9ddb94',\n",
       " 'f02d6986d2754630adba733d4f36ac3a',\n",
       " '29b698868452402db7e7c1db363dc56e',\n",
       " '55698f53bd6144b2b72f8d3ecd616220',\n",
       " 'f81ba4e8a34d4e79b9b2d6af4a5d380a',\n",
       " 'a9432bbbc3974837856204287f30103c',\n",
       " '00c0763360484f58aeeb9f7adf70eb1c',\n",
       " '788fc9e177f14805b8abb7c2d4ad89c2',\n",
       " 'a04307b51e02464a93cd60bb97e98a8b',\n",
       " 'c5a82db46ce849c4b3c6c8e13ae23591',\n",
       " 'fe18aa62e5db4329b6efe8b440f1af79',\n",
       " 'f3d19abddb78426daf52f8cf280fa7fa',\n",
       " 'a53d65f65fa84ec9b70b71f64fe224c1',\n",
       " 'b342fec346524bea896846282e247c90',\n",
       " '0317828c5e0c46c3bf212d4c814eb061',\n",
       " 'd192e9d2017e47f19069e81f283fedd4',\n",
       " '64d35b04efd2435986ffba9c338510d4',\n",
       " 'a03ec53aaaf24fc28f2ce0c572b6a761',\n",
       " '7d652fd4f8d043aca78e37342b977b6f',\n",
       " '6801c10910a942a3b3c267d84fda52dc',\n",
       " '089cee8722144b6ba698a3f59909d59c',\n",
       " '9b1820b6be804ac3ad3ec4973587a4aa',\n",
       " 'dffd5f4c6d79404dba6db45eee316bee',\n",
       " 'f631f23d95a84a1f8711ef6f47c7ca9f',\n",
       " '586169ac409a448fbce005664e807d40',\n",
       " '42f0728c6657454fbeaf07ef89b78107',\n",
       " 'a3c7872fca1142488fbf1e9f91408729',\n",
       " 'bbf51bdecb8f4cafbf8be372dc824578',\n",
       " '7474379e42a44be8a1bc515955ef3a35',\n",
       " '04b07c0f3fd8403387049daa19987dab',\n",
       " '5b7c41742c7d44f1b85560e1579bb283',\n",
       " '5417abfc99ee4d0abea0dcfe60f99ddc',\n",
       " '78a204d0c5154d0293ffaccc77411b3c',\n",
       " '25e2e9af46ef4188aaf249be9e886b70',\n",
       " '5a9eeb86ca354cf381fa52e633bdae06',\n",
       " '810e205bc73b4145b9c1b28778f84d8b',\n",
       " '6641d440814341258e762a74ebd7aa4e',\n",
       " '584c13bc29e74fe88d07fdfbf36870b2',\n",
       " '013a59a2341543ec951fedab0f9fb1a1',\n",
       " 'ff75dce686fb45f5a5744c6a639adbd3',\n",
       " '47eef8bfa94e44b6a385125a841f43f9',\n",
       " 'ac3ee240e5cb433d944db70e2b7298cf',\n",
       " 'c94de39146a9478cbdb3858a503ad4d9',\n",
       " 'e62be5a3251f4487b1f1fdc9813a03e0',\n",
       " '85bb5d4fdaae49f58b0049db2c45eb1d',\n",
       " '533aaff94b1f4dafa2118400042ff385',\n",
       " '2c003c779d60471ab86c1079659632df',\n",
       " 'ed02302269564d248e757e2e8f9ddb21',\n",
       " '3d058acd78b3455c884b7ec5be5c3f2a',\n",
       " 'db1d978004e24c228d15d37472df0eb5',\n",
       " 'e2aab192f0ca40a191ce6ab469b263e2',\n",
       " '4f50052d8faf4a41b9dc2e68faffeccf',\n",
       " 'cfc1b992a3f64e99b9cc5fb81e269400',\n",
       " '84f4d1f2a4904be9b6430d34c68ca6d0',\n",
       " '0d9aec95b8b4474c872113b48146568d',\n",
       " '74fd6ce1565c4a3dbb9b956bbdcab553',\n",
       " 'eaac85b637cb419e991c99f0fdb26bd3',\n",
       " '72e85904b7f54868b141458475c04af5',\n",
       " 'b84fc688f1224cd791f3e01ab95791d1',\n",
       " '7feaa5423dbe4ffc8a7fec17af7636e8',\n",
       " '17580aba83334a0ab2e8b140bfc7d94b',\n",
       " '6a43a2f6aa244fb08b5bf6a39015d298',\n",
       " '5322da550fb0403eb114a56b95192fba',\n",
       " '0cfe8468db02449ea5e95b246845d98c',\n",
       " '5b454bdbcb504ede9e30713d494fa3c0',\n",
       " 'a339ac41a26e42ae822b54bdd7abe247',\n",
       " '6758afcface9427e93a35b24d13ae29e',\n",
       " '008460bec9924a5d92d14fee460dc33e',\n",
       " 'c3643d384ad3480481848f0fca86bc81',\n",
       " '1625b4468170410ab0cf5eea37c1862a',\n",
       " '7acb4c6d180645fb89ed5f54d1a0693a',\n",
       " '8d3540a4dafa4aebb771c88a29ded082',\n",
       " '765a945d3b7948f3b172d2e7bc9c1332',\n",
       " 'd17516b2cf1b476dada1a5e022b4690e',\n",
       " 'a659863ae2fb495595857ab93a422928',\n",
       " '76aef540e51743359d8a3dce0a675805',\n",
       " 'c9993812e7f343679e1411f6e1330f78',\n",
       " '3bfda3189d5a42c1b2ed1f87ffa68fe5',\n",
       " 'bf932d55131740178e838b0c848dbec6',\n",
       " '33e5fb9dea2445ca85929c97260f78c4',\n",
       " 'b547bed4f8aa464ebd84eef00a4ba571',\n",
       " '499a966aab3f4a44a7b66c0882ab27a1',\n",
       " '5d18d257a12b444499645b84cf4e6162',\n",
       " '87a709c0ee484ee788e94809486f00c4',\n",
       " 'f9f5a8965d9a40c582a932ad5e82a6a1',\n",
       " '43eebc65b11440ba88a2e8023a518230',\n",
       " 'a7dbf96683984282a4d809390c579fec',\n",
       " 'e20cfdd994c3497e8a78ce89c25b64a9',\n",
       " '287d6adbad6740bda20604e6073ee0c2',\n",
       " '4c9f97ccdbb74035880403f887eb5fd2',\n",
       " '101a70b5159c4302b55477b160a9a488',\n",
       " '6c4a2c3427464a8195f629b067b97447',\n",
       " '85ec5f8bea894e5f8f2c04aab343a0bf',\n",
       " '44a07062ae024c41a3d10ce17eb3a5a2',\n",
       " '1ccc5be1665a4d7db81140a601bd8b4a',\n",
       " '32c33cb3883340d784e95883885c3ace',\n",
       " '045aa60bf0f944c08c30428d66fabbaa',\n",
       " 'e08119f9efc34982821401aa14267940',\n",
       " 'a893e333fcba4cbebd7a2cecf6292a81',\n",
       " '8b2f8995a6db461faf3c406f9329952e',\n",
       " '43e2fb5ebe274cb5a0c6a85b68cacb19',\n",
       " '0c7fec240ceb48f1b24c423c619c98c3',\n",
       " 'c3f64c5e5db44c09bffd5e1c02b3a999',\n",
       " 'cb3697618ad149908375d8db791749dd',\n",
       " '3b870149e77140909af0aad6f1e7e5d6',\n",
       " '4d7b9117604c4eb4beeedf63871f5145',\n",
       " '46dd048a6719470d9b78cd52b6909c0c',\n",
       " '1dd17d629f35458d8c9b3f45e9571d9b',\n",
       " '00d9509e00da4bcba759d05e5396bb2a',\n",
       " '27aee47c6c7048c5bbe644575d3c15cc',\n",
       " '24873b9a0c364bb9a4dfbe194cc7ad49',\n",
       " '1798c266a5b946e6aade724a5f391c72',\n",
       " '8d40642675d744be991247173824334d',\n",
       " '2e9da7d0681d44318608cb79779c7612',\n",
       " '5e0e08027cdf4e8c9912ebe7f6acc956',\n",
       " 'f301d255965a4d7c95521a899e1f8a83',\n",
       " '6b203894e052450f9d7b8111d9fdb7d2',\n",
       " '385e05137c974bb09e2bbd069340c16c',\n",
       " '207f519680244cc0b69cd921157135bc',\n",
       " '27f34f841cde4d5787a71827cf38fd4c',\n",
       " '03e41ab3a36d46f9bf659b77dd3d6580',\n",
       " 'b5be1a4dee4a4f5aad75bc7d92320485',\n",
       " '46167bbaa48241a89e84642f6d5695ec',\n",
       " '5e967ed8e78142798fdd7af5c42923d4',\n",
       " 'd2d2f5b1637a42e9956a9270ef634927',\n",
       " 'f7b098db7dd9483faec654377bd55d00',\n",
       " '835499f21efc4f63b22f0a5794ab544e',\n",
       " '2d316a9329894849859af03e567c68d2',\n",
       " '34b9c363fccd4d68bc986649fdca64e2',\n",
       " 'fb95b421cae444cf842b47f2185caa44',\n",
       " 'c1706dd4d72c4c258ae6f956fee830b6',\n",
       " 'fd8ce29547d64c4f9441b8519b9ad8cb',\n",
       " '8a9a1885a9494a1c892ae5a4c0c2623c',\n",
       " '5fdee51c969649279252a0816fe0986d',\n",
       " '8f8fb6dd08ba4be3a6a878f74a00bbbb',\n",
       " 'f4e7e26a420e4685b7075e2671f92bc5',\n",
       " '836e5865b5b741aa8998bd8cbf4201f8',\n",
       " 'b8e4fadc7ef4422aa5ba6aa1d0cddfd3',\n",
       " '7438d6994de14e4fa5c363f9fad66286',\n",
       " '712b421499dc4c8196db199817ea8ea4',\n",
       " '2879f454230a4a35ac05bb89a604bd69',\n",
       " '78c9a5c31eb248ac84f887ae4d04ca89',\n",
       " '01ded0eb64ef49d786bf062bc3320422',\n",
       " 'a1ee2f2bb6af4279b95672672614f09e',\n",
       " 'bf4d5ce531864bbb91c1c7375d012b7d',\n",
       " 'f057a3e1027d4fcd91381e786d0b925e',\n",
       " 'f09a0dec4e6d421c8a5bf77d03388d77',\n",
       " '8ef5ec8192674d0788dd85acd174c136',\n",
       " '04e0540dfa574f74bffa541142181716',\n",
       " 'b97787c3c4ce48a9a3b55f9c9b5a5d95',\n",
       " '04d010c8df384d869004d936917c2689',\n",
       " 'c6a62fa3db1c42c6aec1479a26bc09b0',\n",
       " '2b319c5eebec46b1990f6ec029700405',\n",
       " '79add10f8539493e9b9f6593f26582cc',\n",
       " '73b00bbf678042a091586ed23748e097',\n",
       " '185bab3ee6b54690b4213b48bd71c350',\n",
       " 'ba4c8b48ad6f4ee588ad1c67f9add142',\n",
       " '745b57ea75534a1d8bb01b7b05c0ce9b',\n",
       " '393a6169ae75455d9530038d321c28d0',\n",
       " '9dd9674ea0b8402aaabd083ae42a49d0',\n",
       " 'c5d4127e5cde4003934fba8b020c7007',\n",
       " '7139f9448c9e4fe8a3ea0a6856c0bcbf',\n",
       " 'bff11152d6a846c59e7969f0abd6cc01',\n",
       " '6fc93e5ffbb642259f7c0f2f22245c57',\n",
       " '16f7af9ebf034a5b90efc87ce71be603',\n",
       " '2d9d1619101341d5b7c790a8c9877734',\n",
       " 'bb41cc6ca1ba4ae18fad1f8c287325f7',\n",
       " '2eae20b77f854f3da9c0f2870bf41f69',\n",
       " '8fd0f6e4298d43439591f89789ebfb18',\n",
       " '8d8b1444e0eb44cea9f74e756385e844',\n",
       " 'f0e8c9528286468ba200d963aa49f9cb',\n",
       " '6cb4cd4a55194db0b9e4bfbf71683db4',\n",
       " 'c5c66c7956c64b42b514ae38e3e4b6f8',\n",
       " '27ad1a979a76407a801daa2cff49a27d',\n",
       " 'aeb11b205cda4dbc85af28d990655e92',\n",
       " 'a3ee258ae6be45d18e8773ce6d72d0fe',\n",
       " '18f2ded00d4f470984f64412028be9f5',\n",
       " 'ddad7390c1454391b34e01f686884ae7',\n",
       " 'c69d409394cf41d2939ab880f4644ecc',\n",
       " '64713b1c83064bd5aeadf15269790724',\n",
       " 'f6a496c14cee4b2c95cae793a1545b0a',\n",
       " '1cfee2786a534518a159a43065504373',\n",
       " '3c97285040f64127bdc9c5fa54d572cd',\n",
       " '92f6ca20b2194e12a9e7d0c6d4e543ef',\n",
       " 'ee4cca0144674df09e17eb0d8b9a2438',\n",
       " '70dbf4e156ba45d88e5cf9bf9d527b4d',\n",
       " 'aeb6a3a5b3ad42df8217d1dc42aebfb4',\n",
       " '2c05f8b943ae4099aa9d5e617ed0bfd9',\n",
       " 'df80e99f23c04c1ca6bc939bbedcbbb7',\n",
       " 'c245f5997f214993ac0ffc7433ec6990',\n",
       " 'e67c9df71d494bc7b6c9f755c137f77f',\n",
       " '8de7d09e7a964374a2a58ab76805dbf9',\n",
       " 'd50b557a87ff4d1eb6336c02d78b5981',\n",
       " 'dec55bed06334f52848b5ada507795e0',\n",
       " 'dd2ebe3c780e4b9a83795a0ec22d9434',\n",
       " 'dce23432cf5d4a1fad5cbf2b37ca22c3',\n",
       " 'f41029f1041740619ceeb80a06be75ef',\n",
       " 'b82f66c09555457695f6c1e6437058e6',\n",
       " 'aa7f0cc85b19448791bafacba9ada553',\n",
       " 'd17c44e20e47465e9b855ed2235e9ff0',\n",
       " '424010edfd694ff58dc44c556d1548ea',\n",
       " 'c2ef5b2f960241188806dad00eae237e',\n",
       " '8ad1dfe2c0824bb9904b51480c146879',\n",
       " '3a0a788c2e454990b6d2e34e6638271f',\n",
       " '80ba140a54ca409781209a607a012df4',\n",
       " '88d99d1b970248aa8e19321e5caa4ffc',\n",
       " '2bf46c9a7e1549f0bfd902722b6f4148',\n",
       " 'f7873a1173cf40dfba5b822ebee98e57',\n",
       " '392b32732c7540348cb988a1a9a82d97',\n",
       " '1c6612df8eea4c3a91b70dcb70e4ac38',\n",
       " 'c38791ae378f46d49569f1af76bc73d1',\n",
       " '90e17807de824edc9d06343a24203c8b',\n",
       " '0a26ee9cd5f44d2498f8491b3e0aa806',\n",
       " '2fe6aaa86aa9412ba7574dda51728564',\n",
       " '96082f4c6c18456da60b11091ec8b010',\n",
       " 'c5a4a6ac753044829bd011626e13e01b',\n",
       " 'd7b4d1fdd7f74932a608b6a8a0cd7900',\n",
       " '4d1d4430ea2f4aee9b95acfb5a3348ca',\n",
       " 'a03389c10c924e8fba12a7d22bc50a93',\n",
       " '8b7e245a2ecb4c4a86a0a649e3e00883',\n",
       " '191bdab78313486e98d78680e101e66e',\n",
       " '74afd82bb2b24a40a6ff51d76f433df5',\n",
       " '8271cd6de65c45f5bc1a82a007b2b82d',\n",
       " 'cced139bf99c4e8daaf44e8ac257dbe3',\n",
       " '9f10c2f59079467a88d82f64a3820e53',\n",
       " '11e553efe82544708029a517120342cb',\n",
       " '6a55d63a01064640a38f9cf826671f62',\n",
       " '8f2fc21f3d364a149f715474026e0f48',\n",
       " '688224cb025847e58e73f00764078c9b',\n",
       " '7983affd88cb4b04b86f2ab2331efecd',\n",
       " 'c56a079224304dee8cf1c7469bad5de8',\n",
       " '9aa90dd11db1443895518b2531e6dc3e',\n",
       " 'aab321587eb44cc595e9438907ce6e9b',\n",
       " 'e29c8cc6b30a4298b91dd49db843a350',\n",
       " 'fe65e13fd16e4c04b9f130a4c62150b3',\n",
       " '8b6b887961b54dc5a169d889db6688cc',\n",
       " 'a725c23796b24a0d992cbc2741c511b1',\n",
       " '336b42fc91ce4c7c94f4d7fee509b937',\n",
       " '0b0fa050c67f45acb05ccbce88226b75',\n",
       " '52c80d31a6a24d29adfeb1dd54a04ef5',\n",
       " '741aaf217bd04a0cb2957797aa405522']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "client = QdrantClient(\":memory:\")\n",
    "client.get_collections()\n",
    "\n",
    "with open('sklearn.tree.txt', 'r', encoding='utf-8') as file:\n",
    "    documents = file.readlines()\n",
    "\n",
    "# Remove newline characters from each document\n",
    "documents = [doc.strip() for doc in documents]\n",
    "\n",
    "# Now, add the documents to the Qdrant client\n",
    "client.add(\n",
    "    collection_name=\"knowledge-base\",\n",
    "    documents=documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Set the OPENAI_API_KEY environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"#####\"  # Replace with your actual API key\n",
    "\n",
    "async_ai = AsyncOpenAI()\n",
    "\n",
    "async def rag( chat_history: list[str], question: str,  n_points: int = 3) -> str:\n",
    "    results = client.query(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "    \n",
    "    context = \"\\n\".join(r.document for r in results)\n",
    "\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a helpful machine learning bot.\n",
    "    Answer the following question using the provided context.\n",
    "    Also refer to the chat history while answering a question. consider info given by the assisstant only as truth.\n",
    "    The context provided is only documentation for referring information. When asked direct questions about model building remember answers in the chat history and when asked factual question about the docs explicitly, refer to the context documentation. \n",
    "    If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "    If you have limited information on something, state is and then answer \"this is all I know.\" irrespective of how much their word count expectation is.\n",
    "\n",
    "    Question: {question}\n",
    "    CHAT HISTORY : {chat_history}\n",
    "    Context:\n",
    "    {context.strip()}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    completion = await async_ai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": metaprompt},\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The DecisionTreeClassifier in scikit-learn has several parameters that can be tuned to control the behavior of the tree. Some of the important parameters are:\n",
      "\n",
      "1. criterion: This parameter specifies the function to measure the quality of a split. It can be set to \"gini\" for the Gini impurity or \"entropy\" for the information gain.\n",
      "\n",
      "2. splitter: This parameter determines the strategy used to choose the split at each node. It can be set to \"best\" to choose the best split or \"random\" to choose the best random split.\n",
      "\n",
      "3. max_depth: This parameter controls the maximum depth of the tree. A higher value will allow the tree to capture more complex relationships, but it may also lead to overfitting.\n",
      "\n",
      "4. min_samples_split: This parameter sets the minimum number of samples required to split an internal node. If the number of samples is less than this value, the node will not be split.\n",
      "\n",
      "5. min_samples_leaf: This parameter sets the minimum number of samples required to be at a leaf node. If the number of samples is less than this value, the leaf node will be pruned.\n",
      "\n",
      "6. max_features: This parameter determines the number of features to consider when looking for the best split. It can be set to an integer value or a fraction of the total number of features.\n",
      "\n",
      "7. class_weight: This parameter assigns weights to different classes. It can be set to \"balanced\" to automatically adjust the weights based on the class frequencies.\n",
      "\n",
      "These are just some of the parameters available in the DecisionTreeClassifier. For a complete list of parameters and their descriptions, you can refer to the scikit-learn documentation for DecisionTreeClassifier.\n"
     ]
    }
   ],
   "source": [
    "print(await rag([] , \"What all are the parameters for decision tree?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM stands for Support Vector Machine. It is a supervised machine learning algorithm used for classification and regression tasks. SVM finds a hyperplane that separates classes by maximizing the margin between them. It is often used for binary classification problems but can also be extended to multi-class classification.\n"
     ]
    }
   ],
   "source": [
    "print(await rag([],\"What is svm?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "print(await rag([],\"what is photosynthesis?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a branch of artificial intelligence that involves creating models and algorithms that can learn from and make predictions or decisions based on data. It is a process of training a machine to perform a task by providing it with a labeled dataset, which consists of input data and corresponding output labels. The machine analyzes this data to identify patterns, relationships, and trends. Once the model is trained, it can be used to make predictions or decisions on new, unseen data.\n",
      "\n",
      "One commonly used algorithm in machine learning is the decision tree classifier. It is a supervised learning algorithm that creates a tree-like model of decisions and their possible consequences. The model is built by recursively splitting the data based on various features to maximize the predictive accuracy.\n",
      "\n",
      "In the context of scikit-learn, the decision tree classifier is implemented in the sklearn.tree.DecisionTreeClassifier class. This class provides various parameters and methods for customizing and training the decision tree model. The training set, consisting of input features (X) and corresponding labels (y), is used to fit the model. The decision tree classifier considers different splitting criteria, such as Gini impurity or information gain, to determine the best split at each internal node of the tree.\n",
      "\n",
      "Once the model is trained, it can be used to make predictions on new data by calling the predict() method of the DecisionTreeClassifier class. This method takes the input features of the new data and outputs the predicted label.\n",
      "\n",
      "It's important to note that while decision trees are powerful and interpretable models, they can suffer from overfitting if not properly regularized. Techniques such as pruning, limiting the maximum depth of the tree, or using an ensemble of trees can help mitigate this issue.\n",
      "\n",
      "This is all I know about machine learning using the scikit-learn DecisionTreeClassifier class.\n"
     ]
    }
   ],
   "source": [
    "print(await rag([], \"Explain everything you know about machine learning?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.2.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.1.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='bc4c57b8-d1fa-4f87-acc2-0ee21efcddbc'>\n",
       "  <div id=\"f6678640-c7fd-4edd-bb32-4f7f544e9a8b\" data-root-id=\"bc4c57b8-d1fa-4f87-acc2-0ee21efcddbc\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"c057dc43-01ac-4a02-92f5-525cf6a69cef\":{\"version\":\"3.2.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"bc4c57b8-d1fa-4f87-acc2-0ee21efcddbc\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"69c0bcdb-ea08-4198-bbc2-b16fd75771e1\",\"attributes\":{\"plot_id\":\"bc4c57b8-d1fa-4f87-acc2-0ee21efcddbc\",\"comm_id\":\"ea54c43731a84b76b41e3ccba8d12c50\",\"client_comm_id\":\"fa043babc4c643588cb12150ec1a0948\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"c057dc43-01ac-4a02-92f5-525cf6a69cef\",\"roots\":{\"bc4c57b8-d1fa-4f87-acc2-0ee21efcddbc\":\"f6678640-c7fd-4edd-bb32-4f7f544e9a8b\"},\"root_ids\":[\"bc4c57b8-d1fa-4f87-acc2-0ee21efcddbc\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  const is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version && !is_dev) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "bc4c57b8-d1fa-4f87-acc2-0ee21efcddbc"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c37d59f535642ba8d06e59f4316dba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'76bc7749-7d83-44fe-90fd-4a0cd74c6da0': {'version…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.application:Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x107db4410>>, <Task finished name='Task-5' coro=<ParamMethod._eval_async() done, defined at /Users/snehadharne/anaconda3/lib/python3.11/site-packages/panel/param.py:815> exception=TypeError(\"rag() missing 1 required positional argument: 'question'\")>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
      "    ret = callback()\n",
      "          ^^^^^^^^^^\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/tornado/ioloop.py\", line 762, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/panel/param.py\", line 838, in _eval_async\n",
      "    raise e\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/panel/param.py\", line 835, in _eval_async\n",
      "    self._update_inner(await awaitable)\n",
      "                       ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/param/_async.py\", line 11, in _depends\n",
      "    return await func(*args, **kw)  # noqa: E999\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/panel/depends.py\", line 244, in wrapped\n",
      "    return await evaled\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_38183/3925370243.py\", line 5, in collect_messages\n",
      "    response = await rag( chat_history)\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: rag() missing 1 required positional argument: 'question'\n"
     ]
    }
   ],
   "source": [
    "async def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    query.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = await rag( chat_history) \n",
    "    chat_history.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    chat_history.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    response = await rag(context, chat_history) \n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)\n",
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "query = []\n",
    "panels = [] # collect display \n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_38183/1718345190.py:20: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(lambda change: asyncio.create_task(on_submit(change)))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef15e86b901b40ac9f0b716889f6f630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2addacbfec18481c9b144b2423b0a3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what is gini index?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdd1c9b826b4b8d9d5af83c24f0edf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> [\\'The Gini index is a measure of impurity or inequality…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "async def on_submit(_):\n",
    "    question = input_box.value\n",
    "    \n",
    "    \n",
    "    response = await asyncio.gather(rag(question, chat_history))\n",
    "    chat_history.append({'role':'user', 'content':f\"{question}\"})\n",
    "    chat_history.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    input_box.value = \"\"\n",
    "    display(widgets.HTML(f'<b>User:</b> {question}'))\n",
    "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {response}'))\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "print(\"Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(lambda change: asyncio.create_task(on_submit(change)))\n",
    "\n",
    "display(input_box)\n",
    "\n",
    "await asyncio.gather(asyncio.sleep(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'what is gini index?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"['The Gini index is a metric used in decision tree algorithms to measure the impurity or inequality of a node. It calculates the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node. The Gini index ranges from 0 to 1, where a value of 0 represents a perfectly pure node and a value of 1 represents a completely impure node.']\"},\n",
       " {'role': 'user', 'content': 'i used breast-cancer-wisconsin dataset'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"['The Gini index is a metric used in decision tree algorithms to measure the impurity or inequality of a node. It calculates the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node. The Gini index ranges from 0 to 1, where a value of 0 represents a perfectly pure node and a value of 1 represents a completely impure node.']\"},\n",
       " {'role': 'user', 'content': 'which dataset did I use?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"['The Gini index is a metric used in decision tree algorithms to measure the impurity or inequality of a node. It calculates the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node. The Gini index ranges from 0 to 1, where a value of 0 represents a perfectly pure node and a value of 1 represents a completely impure node.']\"},\n",
       " {'role': 'user', 'content': 'what should i set the value for criterion?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"['You used the breast-cancer-wisconsin dataset.']\"}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "def to_numeric(F6):\n",
    "    if F6 == \"?\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return int(F6)\n",
    "\n",
    "def file_preprocess(filename):\n",
    "  df = pd.read_csv(filename)\n",
    "  df['F6'] = df['F6'].apply(to_numeric)\n",
    "  mean_F6 = df['F6'].mean()\n",
    "  df['F6'] = df['F6'].fillna(mean_F6)\n",
    "  return df\n",
    "def runner(json_file):\n",
    "  # Load parameters from JSON\n",
    "  with open(json_file, 'r') as file:\n",
    "      parameters = json.load(file)\n",
    "\n",
    "  model_name = parameters['model_name']\n",
    "  df = file_preprocess(parameters['filename'])\n",
    "\n",
    "  #Check for target_variable is present or not\n",
    "  target_variable = parameters.get(\"target_variable\", None)\n",
    "  if target_variable is None:\n",
    "      raise ValueError(\"Target variable not specified in the parameters.\")\n",
    "\n",
    "  X = df.drop(columns=[target_variable])\n",
    "  y = df[target_variable]\n",
    "\n",
    "  # Define default parameters for SVMClassifier\n",
    "  default_lr_parameters = {\n",
    "      \"penalty\": 'l2',\n",
    "      \"dual\": False,\n",
    "      \"tol\": 0.0001,\n",
    "      \"C\": 1.0,\n",
    "      \"fit_intercept\": True,\n",
    "      \"intercept_scaling\": 1,\n",
    "      \"class_weight\": None,\n",
    "      \"random_state\": None,\n",
    "      \"solver\": 'lbfgs',\n",
    "      \"max_iter\": 100,\n",
    "      \"multi_class\": 'auto',\n",
    "      \"verbose\": 0,\n",
    "      \"warm_start\": False,\n",
    "      \"n_jobs\": None,\n",
    "      \"l1_ratio\": None\n",
    "  }\n",
    "  default_svm_parameters = {\n",
    "    'C': 1.0,\n",
    "    'kernel': 'rbf',\n",
    "    'degree': 3,\n",
    "    'gamma': 'scale',\n",
    "    'coef0': 0.0,\n",
    "    'shrinking': True,\n",
    "    'probability': False,\n",
    "    'tol': 0.001,\n",
    "    'cache_size': 200,\n",
    "    'class_weight': None,\n",
    "    'verbose': False,\n",
    "    'max_iter': -1,\n",
    "    'decision_function_shape': 'ovr',\n",
    "    'break_ties': False,\n",
    "    'random_state': None\n",
    "  }\n",
    "  default_decision_tree_parameters = {\n",
    "    \"criterion\": \"gini\",\n",
    "    \"splitter\": \"best\",\n",
    "    \"max_depth\": None,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"max_features\": None,\n",
    "    \"random_state\": None,\n",
    "    \"max_leaf_nodes\": None,\n",
    "    \"min_impurity_decrease\": 0.0,\n",
    "    \"class_weight\": None,\n",
    "    \"ccp_alpha\": 0.0\n",
    " }\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=parameters['split'], random_state=42)\n",
    "\n",
    "  def_param = {\n",
    "      \"decision_tree\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
    "      \"svm\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
    "      \"lr\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"LogisticRegression\"}\n",
    "  }\n",
    "\n",
    "  param = def_param[model_name][\"param_dict\"]\n",
    "  lib_name = def_param[model_name][\"lib_name\"]\n",
    "  # print(param, lib_name)\n",
    "\n",
    "  # Merge default and user-provided parameters\n",
    "  merged_parameters = {**eval(param), **parameters.get(\"param\", {})}\n",
    "  # print(merged_parameters)\n",
    "\n",
    "  # Initialize the Decision Tree model with the merged parameters\n",
    "  model = eval(lib_name)(**merged_parameters)\n",
    "\n",
    "  # Train the Decision Tree model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  print(f\"Accuracy: {accuracy}\")\n",
    "  return accuracy\n",
    "#   print(merged_parameters)\n",
    "  # print(param)\n",
    "  # print(eval(param))\n",
    "  # print(parameters[\"param\"])\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import json\n",
    "# def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=model,\n",
    "#         messages=messages,\n",
    "#         temperature=0, # this is the degree of randomness of the model's output\n",
    "#     )\n",
    "\n",
    "async def completion1(prompt):\n",
    "    completion = await async_ai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages= prompt,\n",
    "    temperature = 0)\n",
    "    return(completion.choices[0].message.content) \n",
    "\n",
    "   \n",
    "async def make_json(chat_history1):\n",
    "    prompt = f\"\"\"\n",
    "    read the chat history between the user and the chatbot and create a dictionary of the model parameters finalized by them. include filename and append the dataset filename with .csv extension. create  a dictionary named param (which would be the parameters of the model) and write all the parameters asked by the user in the datatype of what the function requires. \n",
    "    include 'target_variable' as mentioned in the prompt. and 'split' should be 0.2 unless some other value is specified. an example for svm model might look like this (with curly brackets instead of sqauare brackets) also make sure you write the model name compatible to the sklearn libraries:\n",
    "    [\n",
    "    \n",
    "    \"filename\" : \"breast-cancer-wisconsin.csv\",\n",
    "    \"model_name\" : \"decision_tree\",\n",
    "    \"param\": [\n",
    "        \"kernel\": \"linear\"\n",
    "    ],\n",
    "    \"target_variable\": \"Class\",\n",
    "    \"split\" : 0.2\n",
    "    ]\n",
    "    text = ```{chat_history1}```\n",
    "    \"\"\"\n",
    "    json_objects = await completion1(prompt)\n",
    "    print(json_objects)\n",
    "    data_dict = json.loads(json_objects)\n",
    "    json_file_path = \"sample.json\"\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file,indent=2)\n",
    "    result = runner(\"sample.json\")\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_46607/3627884150.py:24: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(lambda change: asyncio.create_task(on_submit(change)))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1dbf3c8fc4448c9b124b2739f3d3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3008ae51c83644119d4f3b067c461ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> hey! what is gini index?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209b7aadc5a246beb0695650a32f22ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> [\"I don\\'t know the answer to that question. Can you ple…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d8c3577fd44e17982cf269910f0a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what are the criterons for decision tree?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4797d00e7c0140bc84c799ec5086bb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> [\\'The Gini index is a measure of impurity or diversity …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a5aad527e7477fb7f05dd5232553de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> build a model for breast-cancer-wisconsin dataset and print the accuracy')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af297442cce44e679a31d20c78a53ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> [\\'The criteria for the decision tree algorithm includes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-27' coro=<on_submit() done, defined at /var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_46607/3627884150.py:4> exception=BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \\'\\\\\\'\\\\\\\\n    read the chat history between the user and the chatbot and create a dictionary of the model parameters finalized by them. include filename and append the dataset filename with .csv extension. create  a dictionary named param (which would be the parameters of the model) and write all the parameters asked by the user in the datatype of what the function requires. \\\\\\\\n    include \\\\\\\\\\\\\\'target_variable\\\\\\\\\\\\\\' as mentioned in the prompt. and \\\\\\\\\\\\\\'split\\\\\\\\\\\\\\' should be 0.2 unless some other value is specified. an example for svm model might look like this (with curly brackets instead of sqauare brackets) also make sure you write the model name compatible to the sklearn libraries:\\\\\\\\n    [\\\\\\\\n    \\\\\\\\n    \"filename\" : \"breast-cancer-wisconsin.csv\",\\\\\\\\n    \"model_name\" : \"decision_tree\",\\\\\\\\n    \"param\": [\\\\\\\\n        \"kernel\": \"linear\"\\\\\\\\n    ],\\\\\\\\n    \"target_variable\": \"Class\",\\\\\\\\n    \"split\" : 0.2\\\\\\\\n    ]\\\\\\\\n    text = ```[{\\\\\\\\\\\\\\'role\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'user\\\\\\\\\\\\\\', \\\\\\\\\\\\\\'content\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'hey! what is gini index?\\\\\\\\\\\\\\'}, {\\\\\\\\\\\\\\'role\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'assistant\\\\\\\\\\\\\\', \\\\\\\\\\\\\\'content\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'[\"I don\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'t know the answer to that question. Can you please provide more information or ask a specific question?\"]\\\\\\\\\\\\\\'}, {\\\\\\\\\\\\\\'role\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'user\\\\\\\\\\\\\\', \\\\\\\\\\\\\\'content\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'what are the criterons for decision tree?\\\\\\\\\\\\\\'}, {\\\\\\\\\\\\\\'role\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'assistant\\\\\\\\\\\\\\', \\\\\\\\\\\\\\'content\\\\\\\\\\\\\\': \"[\\\\\\\\\\\\\\'The Gini index is a measure of impurity or diversity used in the context of decision trees. It is calculated for each node in a decision tree and represents the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in that node. The Gini index ranges from 0 to 1, where 0 indicates perfect purity (all elements belong to the same class) and 1 indicates the maximum impurity (elements are evenly distributed across different classes). The decision tree algorithm uses the Gini index to determine the best split for each node, maximizing information gain and creating a more accurate tree.\\\\\\\\\\\\\\']\"}, {\\\\\\\\\\\\\\'role\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'user\\\\\\\\\\\\\\', \\\\\\\\\\\\\\'content\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'build a model for breast-cancer-wisconsin dataset and print the accuracy\\\\\\\\\\\\\\'}, {\\\\\\\\\\\\\\'role\\\\\\\\\\\\\\': \\\\\\\\\\\\\\'assistant\\\\\\\\\\\\\\', \\\\\\\\\\\\\\'content\\\\\\\\\\\\\\': \"[\\\\\\\\\\\\\\'The criteria for the decision tree algorithm includes the Gini index as one of the measurement of impurity or diversity. The Gini index is calculated for each node in a decision tree and represents the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in that node. It ranges from 0 to 1, where 0 indicates perfect purity (all elements belong to the same class) and 1 indicates the maximum impurity (elements are evenly distributed across different classes). The decision tree algorithm uses the Gini index to determine the best split for each node, maximizing information gain and creating a more accurate tree.\\\\\\\\\\\\\\']\"}]```\\\\\\\\n    \\\\\\' is not of type \\\\\\'array\\\\\\' - \\\\\\'messages\\\\\\'\\', \\'type\\': \\'invalid_request_error\\', \\'param\\': None, \\'code\\': None}}')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_46607/3627884150.py\", line 8, in on_submit\n",
      "    accuracy = await make_json(chat_history)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_46607/3298584788.py\", line 151, in make_json\n",
      "    json_objects = await completion1(prompt)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_46607/3298584788.py\", line 128, in completion1\n",
      "    completion = await async_ai.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1199, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1474, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1275, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/snehadharne/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1318, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': '\\'\\\\n    read the chat history between the user and the chatbot and create a dictionary of the model parameters finalized by them. include filename and append the dataset filename with .csv extension. create  a dictionary named param (which would be the parameters of the model) and write all the parameters asked by the user in the datatype of what the function requires. \\\\n    include \\\\\\'target_variable\\\\\\' as mentioned in the prompt. and \\\\\\'split\\\\\\' should be 0.2 unless some other value is specified. an example for svm model might look like this (with curly brackets instead of sqauare brackets) also make sure you write the model name compatible to the sklearn libraries:\\\\n    [\\\\n    \\\\n    \"filename\" : \"breast-cancer-wisconsin.csv\",\\\\n    \"model_name\" : \"decision_tree\",\\\\n    \"param\": [\\\\n        \"kernel\": \"linear\"\\\\n    ],\\\\n    \"target_variable\": \"Class\",\\\\n    \"split\" : 0.2\\\\n    ]\\\\n    text = ```[{\\\\\\'role\\\\\\': \\\\\\'user\\\\\\', \\\\\\'content\\\\\\': \\\\\\'hey! what is gini index?\\\\\\'}, {\\\\\\'role\\\\\\': \\\\\\'assistant\\\\\\', \\\\\\'content\\\\\\': \\\\\\'[\"I don\\\\\\\\\\\\\\'t know the answer to that question. Can you please provide more information or ask a specific question?\"]\\\\\\'}, {\\\\\\'role\\\\\\': \\\\\\'user\\\\\\', \\\\\\'content\\\\\\': \\\\\\'what are the criterons for decision tree?\\\\\\'}, {\\\\\\'role\\\\\\': \\\\\\'assistant\\\\\\', \\\\\\'content\\\\\\': \"[\\\\\\'The Gini index is a measure of impurity or diversity used in the context of decision trees. It is calculated for each node in a decision tree and represents the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in that node. The Gini index ranges from 0 to 1, where 0 indicates perfect purity (all elements belong to the same class) and 1 indicates the maximum impurity (elements are evenly distributed across different classes). The decision tree algorithm uses the Gini index to determine the best split for each node, maximizing information gain and creating a more accurate tree.\\\\\\']\"}, {\\\\\\'role\\\\\\': \\\\\\'user\\\\\\', \\\\\\'content\\\\\\': \\\\\\'build a model for breast-cancer-wisconsin dataset and print the accuracy\\\\\\'}, {\\\\\\'role\\\\\\': \\\\\\'assistant\\\\\\', \\\\\\'content\\\\\\': \"[\\\\\\'The criteria for the decision tree algorithm includes the Gini index as one of the measurement of impurity or diversity. The Gini index is calculated for each node in a decision tree and represents the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in that node. It ranges from 0 to 1, where 0 indicates perfect purity (all elements belong to the same class) and 1 indicates the maximum impurity (elements are evenly distributed across different classes). The decision tree algorithm uses the Gini index to determine the best split for each node, maximizing information gain and creating a more accurate tree.\\\\\\']\"}]```\\\\n    \\' is not of type \\'array\\' - \\'messages\\'', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "async def on_submit(_):\n",
    "    question = input_box.value\n",
    "    \n",
    "    if question == 'done':\n",
    "        accuracy = await make_json(chat_history)\n",
    "        chat_history.append({'role':'user', 'content':\"what is the accuracy?\"})\n",
    "        chat_history.append({'role':'assistant', 'content':\"the accuracy is\"+f\"{accuracy}\"})\n",
    "\n",
    "    response = await asyncio.gather(rag(question, chat_history))\n",
    "    chat_history.append({'role':'user', 'content':f\"{question}\"})\n",
    "    chat_history.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    input_box.value = \"\"\n",
    "    display(widgets.HTML(f'<b>User:</b> {question}'))\n",
    "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {response}'))\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "print(\"Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(lambda change: asyncio.create_task(on_submit(change)))\n",
    "\n",
    "display(input_box)\n",
    "\n",
    "await asyncio.gather(asyncio.sleep(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'what is gini index?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"['The Gini index is a measurement of impurity or inequality in a given set of data. It is commonly used in decision tree algorithms to evaluate the quality of a split. The Gini index ranges from 0 to 1, where 0 represents perfect equality (all elements belong to the same class) and 1 represents perfect inequality (all elements belong to different classes). A lower Gini index indicates a better split.']\"},\n",
       " {'role': 'user',\n",
       "  'content': 'build a model for decision tree classifier with breast-cancer-wisconsin dataset and return the accuracy'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"['The Gini index is a measurement of impurity or inequality in a given set of data. It is commonly used in decision tree algorithms to evaluate the quality of a split. The Gini index ranges from 0 to 1, where 0 represents perfect equality (all elements belong to the same class) and 1 represents perfect inequality (all elements belong to different classes). A lower Gini index indicates a better split.']\"},\n",
       " {'role': 'user',\n",
       "  'content': 'build a model for decision tree classifier with breast-cancer-wisconsin dataset and return the accuracy'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"['The Gini index is a measurement of impurity or inequality in a given set of data. It is commonly used in decision tree algorithms to evaluate the quality of a split. The Gini index ranges from 0 to 1, where 0 represents perfect equality (all elements belong to the same class) and 1 represents perfect inequality (all elements belong to different classes). A lower Gini index indicates a better split.']\"}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8fd72f3a6c4077961c960bc922d10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what is decision tree classfier?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121a87efa5f64d0185960446da29deb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> [\\'The Gini index is a measure of impurity used in decis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ChatCompletion(id='chatcmpl-8Q3MYw520pzfLd6lI6owGGIsZvovT', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The accuracy mentioned in the chat history is 0.985.', role='assistant', function_call=None, tool_calls=None))], created=1701220146, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=13, prompt_tokens=252, total_tokens=265))\n"
     ]
    }
   ],
   "source": [
    "chat_history = [{\"role\": \"user\", \"content\": \"What is the accuracy?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"the accuracy is 0.985\"},]\n",
    "question = \"What is the accuracy?\"\n",
    "response = rag(chat_history, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy we got in the previous run is 0.985.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [{\"role\": \"user\", \"content\": \"what accuracy did the model give?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"the accuracy is 0.985\"},]\n",
    "question = \"What is the accuracy we got in the previous run?\"\n",
    "response = rag(chat_history, question)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##WE WILL WORK THIS OUT TOTALLY TONIGHT\n",
    "\n",
    "#AND IF THIS WORKS I WANT TREAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def to_numeric(F6):\n",
    "    if F6 == \"?\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return int(F6)\n",
    "\n",
    "def file_preprocess(filename):\n",
    "  df = pd.read_csv(filename)\n",
    "  df['F6'] = df['F6'].apply(to_numeric)\n",
    "  mean_F6 = df['F6'].mean()\n",
    "  df['F6'] = df['F6'].fillna(mean_F6)\n",
    "  return df\n",
    "def runner(json_file):\n",
    "  # Load parameters from JSON\n",
    "  with open(json_file, 'r') as file:\n",
    "      parameters = json.load(file)\n",
    "\n",
    "  model_name = parameters['model_name']\n",
    "  df = file_preprocess(parameters['filename'])\n",
    "\n",
    "  #Check for target_variable is present or not\n",
    "  target_variable = parameters.get(\"target_variable\", None)\n",
    "  if target_variable is None:\n",
    "      raise ValueError(\"Target variable not specified in the parameters.\")\n",
    "\n",
    "  X = df.drop(columns=[target_variable])\n",
    "  y = df[target_variable]\n",
    "\n",
    "  # Define default parameters for SVMClassifier\n",
    "  default_lr_parameters = {\n",
    "      \"penalty\": 'l2',\n",
    "      \"dual\": False,\n",
    "      \"tol\": 0.0001,\n",
    "      \"C\": 1.0,\n",
    "      \"fit_intercept\": True,\n",
    "      \"intercept_scaling\": 1,\n",
    "      \"class_weight\": None,\n",
    "      \"random_state\": None,\n",
    "      \"solver\": 'lbfgs',\n",
    "      \"max_iter\": 100,\n",
    "      \"multi_class\": 'auto',\n",
    "      \"verbose\": 0,\n",
    "      \"warm_start\": False,\n",
    "      \"n_jobs\": None,\n",
    "      \"l1_ratio\": None\n",
    "  }\n",
    "  default_svm_parameters = {\n",
    "    'C': 1.0,\n",
    "    'kernel': 'rbf',\n",
    "    'degree': 3,\n",
    "    'gamma': 'scale',\n",
    "    'coef0': 0.0,\n",
    "    'shrinking': True,\n",
    "    'probability': False,\n",
    "    'tol': 0.001,\n",
    "    'cache_size': 200,\n",
    "    'class_weight': None,\n",
    "    'verbose': False,\n",
    "    'max_iter': -1,\n",
    "    'decision_function_shape': 'ovr',\n",
    "    'break_ties': False,\n",
    "    'random_state': None\n",
    "  }\n",
    "  default_decision_tree_parameters = {\n",
    "    \"criterion\": \"gini\",\n",
    "    \"splitter\": \"best\",\n",
    "    \"max_depth\": None,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"max_features\": None,\n",
    "    \"random_state\": None,\n",
    "    \"max_leaf_nodes\": None,\n",
    "    \"min_impurity_decrease\": 0.0,\n",
    "    \"class_weight\": None,\n",
    "    \"ccp_alpha\": 0.0\n",
    " }\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=parameters['split'], random_state=42)\n",
    "\n",
    "  def_param = {\n",
    "      \"decision_tree\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
    "      \"svm\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
    "      \"lr\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"LogisticRegression\"}\n",
    "  }\n",
    "\n",
    "  param = def_param[model_name][\"param_dict\"]\n",
    "  lib_name = def_param[model_name][\"lib_name\"]\n",
    "  # print(param, lib_name)\n",
    "\n",
    "  # Merge default and user-provided parameters\n",
    "  merged_parameters = {**eval(param), **parameters.get(\"param\", {})}\n",
    "  # print(merged_parameters)\n",
    "\n",
    "  # Initialize the Decision Tree model with the merged parameters\n",
    "  model = eval(lib_name)(**merged_parameters)\n",
    "\n",
    "  # Train the Decision Tree model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  print(f\"Accuracy: {accuracy}\")\n",
    "  return accuracy\n",
    "  print(merged_parameters)\n",
    "  # print(param)\n",
    "  # print(eval(param))\n",
    "  # print(parameters[\"param\"])\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = ai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response\n",
    "def make_json(chat_history1):\n",
    "    prompt = f\"\"\"\n",
    "    read the chat history between the user and the chatbot and create a dictionary of the model parameters finalized by them. include filename and append the dataset filename with .csv extension. create  a dictionary named param (which would be the parameters of the model) and write all the parameters asked by the user in the datatype of what the function requires. \n",
    "    include 'target_variable' as mentioned in the prompt. and 'split' should be 0.2 unless some other value is specified. an example for svm model might look like this (with curly brackets instead of sqauare brackets) also make sure you write the model name compatible to the sklearn libraries:\n",
    "    [\n",
    "    \n",
    "    \"filename\" : \"breast-cancer-wisconsin.csv\",\n",
    "    \"model_name\" : \"decision_tree\",\n",
    "    \"param\": [\n",
    "        \"kernel\": \"linear\"\n",
    "    ],\n",
    "    \"target_variable\": \"Class\",\n",
    "    \"split\" : 0.2\n",
    "    ]\n",
    "    text = ```{chat_history1}```\n",
    "    \"\"\"\n",
    "    json_objects = get_completion(prompt)\n",
    "    json_object = json_objects.choices[0].message.content\n",
    "\n",
    "    print(json_objects)\n",
    "    data_dict = json.loads(json_object)\n",
    "    json_file_path = \"sample.json\"\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file,indent=2)\n",
    "    result = runner(\"sample.json\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "ai = OpenAI()\n",
    "def rag(chat_history: list[str], question: str, n_points: int = 3) -> str:\n",
    "    results = client.query(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "    \n",
    "    context = \"\\n\".join(r.document for r in results)\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a helpful machine learning bot.\n",
    "    Answer the following question using the provided context.\n",
    "    Also refer to the chat history while answering a question. consider info given by the assisstant only as truth.\n",
    "    The context provided is only documentation for referring information. When asked direct questions about model building remember answers in the chat history and when asked factual question about the docs explicitly, refer to the context documentation. \n",
    "    If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "    If you have limited information on something, state is and then answer \"this is all I know.\" irrespective of how much their word count expectation is.\n",
    "\n",
    "    Question: {question}\n",
    "    CHAT HISTORY : {chat_history}\n",
    "   Context:\n",
    "    {context.strip()}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    results = ai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful machine learning bot.\"},\n",
    "            {\"role\": \"user\", \"content\": metaprompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/hmsclsfj6y1d_pbb28pqpv_h0000gn/T/ipykernel_46607/3205738789.py:29: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66b6051a716485d858471230dc42e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5ff17b66e843558c5aba43dafcfeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what is log_loss?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16434c54e58640ed93ff50d8434b4781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> Log loss, also known as logarithmic loss or cross-entrop…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647ab36b59614ed09d9311e11f17e819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> build a model with that for breast-cancer-wisconsin dataset with decision tree classi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471638acb1654a3986322e60e52e0cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> To build a decision tree classifier for the breast-cance…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8Q5fFwOTTLjO7Q3y0p2ozMELMf8sN', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='{\\n    \"filename\": \"breast-cancer-wisconsin.csv\",\\n    \"model_name\": \"decision_tree\",\\n    \"param\": {\\n        \"criterion\": \"gini\",\\n        \"splitter\": \"best\"\\n    },\\n    \"target_variable\": \"Class\",\\n    \"split\": 0.2\\n}', role='assistant', function_call=None, tool_calls=None))], created=1701228993, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=66, prompt_tokens=695, total_tokens=761))\n",
      "Accuracy: 0.9714285714285714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e44df437d54b609163a11a55964889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what is the accuracy?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be2280fb52c4f84b64d306fc840649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> The accuracy refers to the mean accuracy of the model on…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bd5921f2fc4258bc686ed904ae57dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what dataset are we using?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954ea05fea946e780fef80e04d7f86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> I don\\'t know what dataset we are using.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fc13451cb740f88f9bbe14132c864f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what dataset did i mention earlier?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5304f1d65f9346e59dae1c4ce3869b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> I don\\'t know what dataset we are using.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f4e68c07474fd483bb17b29480acd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> what dataset did i ask to build the model with?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ad2e3bcc7248b5a5b7484540d197fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> You mentioned earlier that you wanted to build the model…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "def on_submit(_):\n",
    "    query = input_box.value\n",
    "    \n",
    "\n",
    "    if query.lower() == 'exit':\n",
    "        print(\"Thank you for using the State of the Union chatbot!\")\n",
    "        return\n",
    "    if query.lower() == 'done':\n",
    "\n",
    "        human_tex = chat_history\n",
    "        eval_metrics= make_json(human_tex)\n",
    "        chat_history.append(('what is the accuracy?', f'The resulting accuracy is {eval_metrics}'))\n",
    "        # chat_history.append(('what is the resulting accuracy?', 'the resulting accuracy is '+str(eval_metrics)))\n",
    "        return\n",
    "    \n",
    "    response = rag(chat_history, query)\n",
    "    result = response.choices[0].message.content\n",
    "    chat_history.append((query, result))\n",
    "\n",
    "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
    "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result}'))\n",
    "    input_box.value = \"\"\n",
    "\n",
    "print(\"Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "display(input_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('breast-cancer-wisconsin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>776715</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>841769</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>888820</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample  F1  F2  F3  F4  F5  F6  F7  F8  F9  Class\n",
       "0    1000025   5   1   1   1   2   1   3   1   1      2\n",
       "1    1002945   5   4   4   5   7  10   3   2   1      2\n",
       "2    1015425   3   1   1   1   2   2   3   1   1      2\n",
       "3    1016277   6   8   8   1   3   4   3   7   1      2\n",
       "4    1017023   4   1   1   3   2   1   3   1   1      2\n",
       "..       ...  ..  ..  ..  ..  ..  ..  ..  ..  ..    ...\n",
       "694   776715   3   1   1   1   3   2   1   1   1      2\n",
       "695   841769   2   1   1   1   2   1   1   1   1      2\n",
       "696   888820   5  10  10   3   7   3   8  10   2      4\n",
       "697   897471   4   8   6   4   3   4  10   6   1      4\n",
       "698   897471   4   8   8   5   4   5  10   4   1      4\n",
       "\n",
       "[699 rows x 11 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
