{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "def add_svm_documents(client):\n",
    "    with open('svm.txt', 'r', encoding='utf-8') as file:\n",
    "        documents = file.readlines()\n",
    "    documents = [doc.strip() for doc in documents]\n",
    "    client.add(collection_name=\"knowledge-base\", documents=documents)\n",
    "\n",
    "# def add_decision_tree_documents(client):\n",
    "#     with open('decision_tree.txt', 'r', encoding='utf-8') as file:\n",
    "#         documents = file.readlines()\n",
    "#     documents = [doc.strip() for doc in documents]\n",
    "#     client.add(collection_name=\"knowledge-base\", documents=documents)\n",
    "\n",
    "def add_decision_tree_documents(client):\n",
    "    with open('decision_tree.txt', 'r', encoding='utf-8') as file:\n",
    "        documents = file.readlines()\n",
    "    documents = [doc.strip() for doc in documents]\n",
    "    client.add(collection_name=\"knowledge-base\", documents=documents)\n",
    "\n",
    "def add_sklearn_tree_documents(client):\n",
    "    with open('sklearn.tree.txt', 'r', encoding='utf-8') as file:\n",
    "        documents = file.readlines()\n",
    "    documents = [doc.strip() for doc in documents]\n",
    "    client.add(collection_name=\"knowledge-base\", documents=documents)\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "user_input = input(\"Enter the model you want to add documents from (decision_tree, sklearn_tree, etc.): \")\n",
    "\n",
    "if user_input.lower() == 'decision_tree':\n",
    "    add_decision_tree_documents(client)\n",
    "elif user_input.lower() == 'sklearn_tree':\n",
    "    add_sklearn_tree_documents(client)\n",
    "else:\n",
    "    add_svm_documents(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "from openai import OpenAI\n",
    "ai = OpenAI()\n",
    "def rag(chat_history: list[str], question: str, n_points: int = 3) -> str:\n",
    "    results = client.query(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "    \n",
    "    context = \"\\n\".join(r.document for r in results)\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a helpful machine learning bot.\n",
    "    Answer the following question using the provided context.\n",
    "    If the user asks you to build a model : If the parameters are not defined by the user, ask them to specify, a sample json file that is to be to made to run\n",
    "    the model looks like this:\n",
    "    [\n",
    "    \n",
    "    \"filename\" : \"breast-cancer-wisconsin.csv\",\n",
    "    \"model_name\" : \"decision_tree\",\n",
    "    \"param\": [\n",
    "        \"kernel\": \"linear\"\n",
    "    ],\n",
    "    \"target_variable\": \"Class\",\n",
    "    \"split\" : 0.2\n",
    "    ] \n",
    "\n",
    "    if everything is mentioned return the json file\n",
    "    if user says done return -1\n",
    "    Always make sure you are checking this first before giving any response. \n",
    "    Also refer to the chat history while answering a question. consider info given by the assisstant only as truth.\n",
    "    The context provided is only documentation for referring information. When asked direct questions about model building remember answers in the chat history and when asked factual question about the docs explicitly, refer to the context documentation. \n",
    "    If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "    If you have limited information on something, state is and then answer \"this is all I know.\" irrespective of how much their word count expectation is.\n",
    "    \n",
    "    Question: {question}\n",
    "    CHAT HISTORY : {chat_history}\n",
    "   Context:\n",
    "    {context.strip()}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    results = ai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful machine learning bot.\"},\n",
    "            {\"role\": \"user\", \"content\": metaprompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import json\n",
    "def to_numeric(F6):\n",
    "    if F6 == \"?\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return int(F6)\n",
    "\n",
    "def file_preprocess(filename):\n",
    "  df = pd.read_csv(filename)\n",
    "#   df['F6'] = df['F6'].apply(to_numeric)\n",
    "#   mean_F6 = df['F6'].mean()\n",
    "#   df['F6'] = df['F6'].fillna(mean_F6)\n",
    "  return df\n",
    "def runner(json_file, model_file):\n",
    "    # Load parameters from JSON\n",
    "    with open(json_file, 'r') as file:\n",
    "        parameters = json.load(file)\n",
    "    with open(model_file, 'r') as file:\n",
    "        model_param = json.load(file)\n",
    "\n",
    "    model_name = parameters['model_name']\n",
    "    df = file_preprocess(parameters['filename'])\n",
    "\n",
    "    #Check for target_variable is present or not\n",
    "    target_variable = parameters.get(\"target_variable\", None)\n",
    "    if target_variable is None:\n",
    "      raise ValueError(\"Target variable not specified in the parameters.\")\n",
    "\n",
    "    X = df.drop(columns=[target_variable])\n",
    "    y = df[target_variable]\n",
    "\n",
    "  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=parameters['split'], random_state=42)\n",
    "\n",
    "    def_param = {\n",
    "      \"decision_tree\" : {\"param_dict\" : \"default_decision_tree_parameters\", \"lib_name\" : \"DecisionTreeClassifier\"},\n",
    "      \"svm\" : {\"param_dict\" : \"default_svm_parameters\", \"lib_name\" : \"SVC\"},\n",
    "      \"lr\" : {\"param_dict\" : \"default_lr_parameters\", \"lib_name\" : \"LogisticRegression\"}\n",
    "    }\n",
    "\n",
    "    params= def_param[model_name][\"param_dict\"]\n",
    "    param = model_param[params]\n",
    "    lib_name = def_param[model_name][\"lib_name\"]\n",
    "  # print(param, lib_name)\n",
    "\n",
    "  # Merge default and user-provided parameters\n",
    "    merged_parameters = {**eval(str(param)), **parameters.get(\"param\", {})}\n",
    "  # print(merged_parameters)\n",
    "\n",
    "  # Initialize the Decision Tree model with the merged parameters\n",
    "    model = eval(lib_name)(**merged_parameters)\n",
    "\n",
    "  # Train the Decision Tree model\n",
    "    model.fit(X_train, y_train)\n",
    "    para= model.get_params()\n",
    "  # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "  # Evaluate the model\n",
    "    #accuracy = accuracy_score(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "#     print(f\"Accuracy: {acc}\")\n",
    "    cr = classification_report(y_test,y_pred)\n",
    "#     print(cr)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "#     print(cf_matrix)\n",
    "    \n",
    "    mmy_dict = {\"accuracy\" : str(acc), \"class_report\" : str(cr), \"conf_mat\" : str(cf_matrix),\"paramters\" : str(para)}\n",
    "    print(mmy_dict)\n",
    "    return mmy_dict\n",
    "    #print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    #return accuracy\n",
    "    #print(merged_parameters)\n",
    "  # print(param)\n",
    "  # print(eval(param))\n",
    "  # print(parameters[\"param\"])\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = ai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response\n",
    "def make_json(chat_history1):\n",
    "    prompt = f\"\"\"\n",
    "    read the chat history between the user and the chatbot and create a dictionary of the model parameters finalized by them. include filename and append the dataset filename with .csv extension. create  a dictionary named param (which would be the parameters of the model) and write all the parameters asked by the user in the datatype of what the function requires. \n",
    "    include 'target_variable' as mentioned in the prompt. and 'split' should be 0.2 unless some other value is specified. an example for svm model might look like this (with curly brackets instead of sqauare brackets) also make sure you write the model name compatible to the sklearn libraries:\n",
    "    [\n",
    "    \n",
    "    \"filename\" : \"<file_name>.csv\",\n",
    "    \"model_name\" : \"decision_tree\",\n",
    "    \"param\": [\n",
    "        \"max_depth\": 3\n",
    "    ],\n",
    "    \"target_variable\": \"Class\",\n",
    "    \"split\" : <0.2 or 0.3>\n",
    "    ]\n",
    "    text = ```{chat_history1}```\n",
    "    \"\"\"\n",
    "    json_objects = get_completion(prompt)\n",
    "    # json_objects = rag(chat_history1, 'return the json object')\n",
    "    json_object = json_objects.choices[0].message.content\n",
    "\n",
    "    print(json_object)\n",
    "    data_dict = json.loads(json_object)\n",
    "    json_file_path = \"sample.json\"\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file,indent=2)\n",
    "    result = runner(\"sample.json\",\"model_parameters.json\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/kkd4kch12b35yvx1xk3m4gr00000gn/T/ipykernel_9239/1112985429.py:40: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  input_box.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd1f347ad5d4d1fab622ae41c0f8a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Please enter your question:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06f4ec3846a4786880e82a80d969073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> build a svm model on Iris with target variable = Species and kernel = linear')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33613a1b5d0b418f9adc37f00341da3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> Sure, I can help you build an SVM model on the Iris data…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626a7222fea74a0bbaef7bc3f762c280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> test_train_split ratio = 0.3 and filename = Iris')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297ad48b65924607868cc1fb0cb9b860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> I cannot directly answer the question based on the given…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a406031258384719a9409305a01fd465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> split = 0.3')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324a270f73fb400a824e422058cbbb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> {\\n    \"filename\" : \"breast-cancer-wisconsin.csv\",\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85130a3479444e69510e5143ee247a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> filename = Iris')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066171886a81414a968325d34d5aa5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> I don\\'t know the answer to your question regarding the …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4735a16030453d9cb37c8cbbef270f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> build a svm model on Iris dataset with target variable = Species , split = 0.3 and ke…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204e9187352645118eb94dc89d95448f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> {\\n    \"filename\" : \"Iris\",\\n    \"model_name\" : \"svm\",\\n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"filename\": \"Iris.csv\",\n",
      "    \"model_name\": \"svm\",\n",
      "    \"param\": {\n",
      "        \"kernel\": \"linear\"\n",
      "    },\n",
      "    \"target_variable\": \"Species\",\n",
      "    \"split\": 0.3\n",
      "}\n",
      "{'accuracy': '1.0', 'class_report': '                 precision    recall  f1-score   support\\n\\n    Iris-setosa       1.00      1.00      1.00        19\\nIris-versicolor       1.00      1.00      1.00        13\\n Iris-virginica       1.00      1.00      1.00        13\\n\\n       accuracy                           1.00        45\\n      macro avg       1.00      1.00      1.00        45\\n   weighted avg       1.00      1.00      1.00        45\\n', 'conf_mat': '[[19  0  0]\\n [ 0 13  0]\\n [ 0  0 13]]', 'paramters': \"{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b292df68cbf5430c8ca9ae762cdb3495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>User:</b> please return -1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4de68ba228f4c9fa06b7f60bcb5a2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b><font color=\"blue\">Chatbot:</font></b> The following are the model params and evaluation metric…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "chat_history = []\n",
    "\n",
    "def on_submit(_):\n",
    "    query = input_box.value\n",
    "    \n",
    "\n",
    "    if query.lower() == 'exit':\n",
    "        print(\"Thank you for using the State of the Union chatbot!\")\n",
    "        return\n",
    "    # if query.lower() == 'done':\n",
    "\n",
    "    #     human_tex = chat_history\n",
    "    #     eval_metrics= make_json(human_tex)\n",
    "    #     chat_history.append(('what is the accuracy?', f'The resulting accuracy is {eval_metrics}'))\n",
    "    #     # chat_history.append(('what is the resulting accuracy?', 'the resulting accuracy is '+str(eval_metrics)))\n",
    "    #     return\n",
    "    \n",
    "    response = rag(chat_history, query)\n",
    "    result = response.choices[0].message.content\n",
    "    if result == '-1':\n",
    "        chat_history.append((query, 'building model'))\n",
    "        # chat_history.append(('what is the resulting accuracy?', 'the resulting accuracy is '+str(eval_metrics)))\n",
    "        human_tex = chat_history\n",
    "        eval_metrics= make_json(human_tex)\n",
    "        # chat_history.append(('what is the accuracy?', f'The resulting accuracy is {eval_metrics}'))\n",
    "        result = 'The following are the model params and evaluation metrics ' + str(eval_metrics)\n",
    "        # chat_history.append(('what is the resulting accuracy?', 'the resulting accuracy is '+str(eval_metrics)))\n",
    "\n",
    "    chat_history.append((query, result))\n",
    "\n",
    "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
    "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result}'))\n",
    "    input_box.value = \"\"\n",
    "\n",
    "print(\"Welcome to the Transformers chatbot! Type done when you want run the model. Type 'exit' to stop.\")\n",
    "\n",
    "input_box = widgets.Text(placeholder='Please enter your question:')\n",
    "input_box.on_submit(on_submit)\n",
    "\n",
    "display(input_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what is entropy?',\n",
       "  'Entropy is a measure of impurity in a set of data. In the context of machine learning, entropy is used as a criterion to measure the quality of a split when constructing decision trees. It quantifies the amount of uncertainty or randomness in the data.'),\n",
       " ('build a decision tree model with that for breast-cancer-wisconsin dataset, the target variable is Class',\n",
       "  'building model'),\n",
       " ('build a decision tree model with that for breast-cancer-wisconsin dataset, the target variable is Class',\n",
       "  'The resulting accuracy is 0.9571428571428572'),\n",
       " ('maybe build the model again but this time with log loss criterion',\n",
       "  'building model'),\n",
       " ('maybe build the model again but this time with log loss criterion',\n",
       "  'The resulting accuracy is 0.9642857142857143'),\n",
       " ('now again build that model with gini index', 'building model'),\n",
       " ('now again build that model with gini index',\n",
       "  'The resulting accuracy is 0.9642857142857143'),\n",
       " ('can you enlist all the criterea with their corresponding resulting accuracies?',\n",
       "  \"I'm sorry, but I don't have the information to answer your question.\"),\n",
       " ('you can do that by refering to the chat history?',\n",
       "  \"To return the feature importances, you can use the `feature_importances_` attribute of the decision tree model. This attribute provides a measure of the importance of each feature in predicting the target variable. However, since I don't have access to the chat history, I cannot provide you with the specific feature importances for the breast-cancer-wisconsin dataset. You would need to refer to the chat history to see if this information was provided.\"),\n",
       " ('refer to the chat history and enlist all the criterion we tried',\n",
       "  'Based on the chat history, the criteria that were tried for building the decision tree model on the breast-cancer-wisconsin dataset are:\\n\\n1. Entropy criterion\\n2. Log loss criterion\\n3. Gini index criterion\\n\\nThe corresponding resulting accuracies for each criterion are as follows:\\n\\n1. Entropy criterion: 0.9571428571428572\\n2. Log loss criterion: 0.9642857142857143\\n3. Gini index criterion: 0.9642857142857143')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
